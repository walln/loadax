{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Loadax","text":"<p>Loadax is a dataloading library designed for the JAX ecosystem. It provides utilities for feeding data into your training loop without having to worry about batching, shuffling, and other preprocessing steps. Loadax also handles background prefetching to improve performance, and distriubted data loading to train on multiple devices and even multiple hosts.</p> Loadax Example<pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\nloader = DataloaderBuilder(batcher).batch_size(2).build(dataset)\n\nfor batch in loader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add loadax\n</code></pre>"},{"location":"multi-host/","title":"Multi-Host Training","text":"<p>Loadax provides a simple interface for defining your dataloading strategy for distributed training. This means that you can easily train your models on multiple hosts, and load data in parallel across multiple hosts. Loadax also provides a few common sharding configurations that can be used out of the box, but you can also create your own sharding configurations using JAX's <code>Mesh</code> and <code>NamedSharding</code> primitives.</p> <p>Loadax's DistributedDataloader will automatically determine which elements to load on each shard within the network ensuring that the data is evenly distributed, and each node only gets the data it needs. This requires no manual configuration, replication, or network topology changes.</p> Creating a distributed dataloader<pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\nfrom loadax.dataloader.loader import Dataloader\nfrom loadax.sharding_utilities import fsdp_sharding\n\nmesh, axis_names = fsdp_sharding()\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\n\nmesh = Mesh(...)\ndataloader = DataloaderBuilder(batcher)\n    .batch_size(2)\n    .shard(mesh, axis_names[0])\n    .build(dataset)\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Loadax is a work in progress, and there are many features that are planned for the future. The current focus is on building a quality set of integrations with common dataset formats such as HuggingFace, Polars, and SQLite.</p> <p>Here is a list of planned features:</p> <ul> <li>Support for more dataset formats</li> <li>Variable batching</li> <li>Throughput monitoring</li> <li>Better integration with the JAX profiling tools</li> <li>Utilities for working with TPUs</li> </ul> <p>If you have any ideas for new features, please open an issue or submit a pull request.</p>"},{"location":"api/builder/","title":"DataloaderBuilder","text":"<p>Most of the time, you just want to create a dataloader, set a few options, and be done with it. The <code>DataloaderBuilder</code> is a convenient way to do this. It is a fluent builder that allows you to chain together methods to create a dataloader instead of configuring each option individually.</p> Creating a dataloader<pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher).batch_size(2).build(dataset)\n\nfor batch in dataloader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>               Bases: <code>Generic[DatasetItem, Batch]</code></p> <p>A dataloader is a primitive for efficiently loading data from a dataset.</p> <p>A dataloader is effectively a smart iterator that optimizes getting data from the dataset. The dataloader is responsible for ensuring the data is batched, ready when it is needed, and loaded into mmemory in a way that is efficient. All of this should be done without hijacking the main thread and preventing you from doing the important parts of your training loop.</p> <p>The dataloader is built by chaining methods on the <code>DataLoader</code> class. The methods are designed to be chained in a fluent style, allowing you to build a dataloader with the desired configuration.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher).batch_size(2).build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Attributes:</p> Name Type Description <code>batcher</code> <code>Batcher</code> <p>The batcher to use for batching.</p> <code>strategy</code> <code>BatchStrategy</code> <p>The batch strategy to use for computing batches.</p> <code>num_workers</code> <code>int</code> <p>The number of workers to use for parallel data loading.</p> <code>prefetch_factor</code> <code>int</code> <p>The prefetch factor to use for prefetching.</p> <code>sharding_strategy</code> <code>JaxShardingStrategy</code> <p>The sharding strategy to use for distributed data loading.</p> <code>shard_id</code> <code>int</code> <p>The ID of the shard to load the data from.</p> <code>num_shards</code> <code>int</code> <p>The number of shards to distribute the data across.</p> <p>A dataloader is effectively a smart iterator that optimizes getting data from the dataset. The dataloader is responsible for ensuring the data is batched, ready when it is needed, and loaded into mmemory in a way that is efficient. All of this should be done without hijacking the main thread and preventing you from doing the important parts of your training loop.</p> <p>The dataloader is built by chaining methods on the <code>DataLoader</code> class. The methods are designed to be chained in a fluent style, allowing you to build a dataloader with the desired configuration.</p> <p>Parameters:</p> Name Type Description Default <code>batcher</code> <code>Batcher</code> <p>The batcher to use for batching.</p> required Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def __init__(self, batcher: Batcher[DatasetItem, Batch]):\n    \"\"\"A dataloader is a primitive for efficiently loading data from a dataset.\n\n    A dataloader is effectively a smart iterator that optimizes getting data from\n    the dataset. The dataloader is responsible for ensuring the data is batched,\n    ready when it is needed, and loaded into mmemory in a way that is efficient.\n    All of this should be done without hijacking the main thread and preventing you\n    from doing the important parts of your training loop.\n\n    The dataloader is built by chaining methods on the `DataLoader` class. The\n    methods are designed to be chained in a fluent style, allowing you to build\n    a dataloader with the desired configuration.\n\n    Args:\n        batcher (Batcher): The batcher to use for batching.\n    \"\"\"\n    self.batcher = batcher\n</code></pre>"},{"location":"api/builder/#loadax.DataloaderBuilder.batch_size","title":"batch_size","text":"<pre><code>batch_size(batch_size: int) -&gt; DataloaderBuilder[DatasetItem, Batch]\n</code></pre> <p>Set the batch size for the dataloader.</p> <p>This method sets the batch size for the dataloader. The batch size is the number of items to include in a batch. The batch size should be a multiple of the number of workers, otherwise the dataloader will not be able to prefetch batches efficiently.</p> <p>Currently, the dataloader only supports fixed batch sizes. This means that with the exception of the last batch, all other batches will have the same size. This may change in the future with the introduction of dynamic batch sizes.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher).batch_size(2).build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size to use for the dataloader.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>DataloaderBuilder[DatasetItem, Batch]</code> <p>The dataloader with the batch size set.</p> Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def batch_size(self, batch_size: int) -&gt; \"DataloaderBuilder[DatasetItem, Batch]\":\n    \"\"\"Set the batch size for the dataloader.\n\n    This method sets the batch size for the dataloader. The batch size is the\n    number of items to include in a batch. The batch size should be a multiple\n    of the number of workers, otherwise the dataloader will not be able to\n    prefetch batches efficiently.\n\n    Currently, the dataloader only supports fixed batch sizes. This means that\n    with the exception of the last batch, all other batches will have the same\n    size. This may change in the future with the introduction of dynamic batch\n    sizes.\n\n    Example:\n        ```python\n        from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = DataloaderBuilder(batcher).batch_size(2).build(dataset)\n        iterator = iter(dataloader)\n        for batch in iterator:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        batch_size (int): The batch size to use for the dataloader.\n\n    Returns:\n        Dataloader: The dataloader with the batch size set.\n    \"\"\"\n    self.strategy = FixedBatchStrategy(batch_size)\n    return self\n</code></pre>"},{"location":"api/builder/#loadax.DataloaderBuilder.workers","title":"workers","text":"<pre><code>workers(num_workers: int) -&gt; DataloaderBuilder[DatasetItem, Batch]\n</code></pre> <p>Set the number of workers for the dataloader.</p> <p>This method sets the number of workers for the dataloader. The number of workers determines the number of parallel threads that will be used to load data. The number of workers should be a multiple of the batch size, otherwise the dataloader will not be able to prefetch batches efficiently.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher).batch_size(2).workers(2).build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>The number of workers to use for the dataloader.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>DataloaderBuilder[DatasetItem, Batch]</code> <p>The dataloader with the number of workers set.</p> Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def workers(self, num_workers: int) -&gt; \"DataloaderBuilder[DatasetItem, Batch]\":\n    \"\"\"Set the number of workers for the dataloader.\n\n    This method sets the number of workers for the dataloader. The number of\n    workers determines the number of parallel threads that will be used to\n    load data. The number of workers should be a multiple of the batch size,\n    otherwise the dataloader will not be able to prefetch batches efficiently.\n\n    Example:\n        ```python\n        from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = DataloaderBuilder(batcher).batch_size(2).workers(2).build(dataset)\n        iterator = iter(dataloader)\n        for batch in iterator:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        num_workers (int): The number of workers to use for the dataloader.\n\n    Returns:\n        Dataloader: The dataloader with the number of workers set.\n    \"\"\"\n    self.num_workers = num_workers\n    return self\n</code></pre>"},{"location":"api/builder/#loadax.DataloaderBuilder.prefetch","title":"prefetch","text":"<pre><code>prefetch(factor: int) -&gt; DataloaderBuilder[DatasetItem, Batch]\n</code></pre> <p>Set the prefetch factor for the dataloader.</p> <p>This method sets the prefetch factor for the dataloader. The prefetch factor determines the number of batches to prefetch ahead of time. The prefetch factor is a multiplier and should be a multiple of the number of workers, otherwise the dataloader will not be able to prefetch batches efficiently.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher)\n                .batch_size(2)\n                .prefetch(2)\n                .build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>int</code> <p>The prefetch factor to use for the dataloader.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>DataloaderBuilder[DatasetItem, Batch]</code> <p>The dataloader with the prefetch factor set.</p> Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def prefetch(self, factor: int) -&gt; \"DataloaderBuilder[DatasetItem, Batch]\":\n    \"\"\"Set the prefetch factor for the dataloader.\n\n    This method sets the prefetch factor for the dataloader. The prefetch\n    factor determines the number of batches to prefetch ahead of time. The\n    prefetch factor is a multiplier and should be a multiple of the number\n    of workers, otherwise the dataloader will not be able to prefetch batches\n    efficiently.\n\n    Example:\n        ```python\n        from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = DataloaderBuilder(batcher)\n                        .batch_size(2)\n                        .prefetch(2)\n                        .build(dataset)\n        iterator = iter(dataloader)\n        for batch in iterator:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        factor (int): The prefetch factor to use for the dataloader.\n\n    Returns:\n        Dataloader: The dataloader with the prefetch factor set.\n    \"\"\"\n    self.prefetch_factor = factor if factor &gt; 0 else 1\n    return self\n</code></pre>"},{"location":"api/builder/#loadax.DataloaderBuilder.shard","title":"shard","text":"<pre><code>shard(mesh: Mesh, data_axis_name: str | None = None, num_shards: int | None = None, shard_id: int | None = None) -&gt; DataloaderBuilder[DatasetItem, Batch]\n</code></pre> <p>Set the mesh and partition spec for the dataloader.</p> <p>This will distribute the dataloading across multiple nodes within the same distributed network. This is useful for training large models on multiple nodes. You can then load the data into multiple devices on each node and train the model in parallel.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher)\n                    .batch_size(2)\n                    .shard(mesh, data_axis_name='data')\n                    .build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Mesh</code> <p>The mesh to use for sharding.</p> required <code>data_axis_name</code> <code>str | None</code> <p>The name of the data axis to use for sharding.</p> <code>None</code> <code>num_shards</code> <code>int</code> <p>The number of shards to distribute the data across. If not specified, the dataloader will choose automatically using jax.process_count().</p> <code>None</code> <code>shard_id</code> <code>int</code> <p>The ID of the shard to load the data from. If not specified, the dataloader will choose automatically using jax.process_index().</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>DataloaderBuilder[DatasetItem, Batch]</code> <p>The dataloader with the mesh and partition spec set.</p> Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def shard(\n    self,\n    mesh: Mesh,\n    data_axis_name: str | None = None,\n    num_shards: int | None = None,\n    shard_id: int | None = None,\n) -&gt; \"DataloaderBuilder[DatasetItem, Batch]\":\n    \"\"\"Set the mesh and partition spec for the dataloader.\n\n    This will distribute the dataloading across multiple nodes within the same\n    distributed network. This is useful for training large models on multiple\n    nodes. You can then load the data into multiple devices on each node and\n    train the model in parallel.\n\n    Example:\n        ```python\n        from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = DataloaderBuilder(batcher)\n                            .batch_size(2)\n                            .shard(mesh, data_axis_name='data')\n                            .build(dataset)\n        iterator = iter(dataloader)\n        for batch in iterator:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        mesh (Mesh): The mesh to use for sharding.\n        data_axis_name (str | None): The name of the data axis to use for sharding.\n        num_shards (int): The number of shards to distribute the data across.\n            If not specified, the dataloader will choose automatically using\n            jax.process_count().\n        shard_id (int): The ID of the shard to load the data from.\n            If not specified, the dataloader will choose automatically using\n            jax.process_index().\n\n    Returns:\n        Dataloader: The dataloader with the mesh and partition spec set.\n    \"\"\"\n    if num_shards and num_shards &lt;= 1:\n        self.sharding_strategy = NoShardingStrategy()\n    else:\n        self.sharding_strategy = DistributedShardingStrategy(\n            mesh, data_shard_axis=data_axis_name\n        )\n    self.shard_id = shard_id\n    self.num_shards = num_shards\n    return self\n</code></pre>"},{"location":"api/builder/#loadax.DataloaderBuilder.build","title":"build","text":"<pre><code>build(dataset: Dataset[DatasetItem]) -&gt; Dataloader[DatasetItem, Batch]\n</code></pre> <p>Construct the dataloader from the current configuration.</p> <p>This method constructs the dataloader from the current configuration. The dataloader is constructed based on the current configuration of the dataloader. If the dataloader is not configured to use multiprocessing, it will use the naive dataloader. If the dataloader is configured to use multiprocessing, it will use the multiprocessing dataloader.</p> <p>The default configuration of the dataloader is to use a fixed batch size of 1 and to use a single worker. This means that there is not parallelism in loading the data, but the data will still be prefetched with 2 batches in advance on the single worker. This is a good default configuration for most use cases, unless you have a fast training loop and potentially many devices.</p> Example <pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = DataloaderBuilder(batcher).batch_size(2).workers(2).build(dataset)\niterator = iter(dataloader)\nfor batch in iterator:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to load data from.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>Dataloader[DatasetItem, Batch]</code> <p>The dataloader constructed from the current configuration.</p> Source code in <code>src/loadax/dataloader/builder.py</code> <pre><code>def build(self, dataset: Dataset[DatasetItem]) -&gt; Dataloader[DatasetItem, Batch]:\n    \"\"\"Construct the dataloader from the current configuration.\n\n    This method constructs the dataloader from the current configuration. The\n    dataloader is constructed based on the current configuration of the dataloader.\n    If the dataloader is not configured to use multiprocessing, it will use the\n    naive dataloader. If the dataloader is configured to use multiprocessing, it\n    will use the multiprocessing dataloader.\n\n    The default configuration of the dataloader is to use a fixed batch size of\n    1 and to use a single worker. This means that there is not parallelism in\n    loading the data, but the data will still be prefetched with 2 batches in\n    advance on the single worker. This is a good default configuration for most\n    use cases, unless you have a fast training loop and potentially many devices.\n\n    Example:\n        ```python\n        from loadax import DataloaderBuilder, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = DataloaderBuilder(batcher).batch_size(2).workers(2).build(dataset)\n        iterator = iter(dataloader)\n        for batch in iterator:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to load data from.\n\n    Returns:\n        Dataloader: The dataloader constructed from the current configuration.\n    \"\"\"\n    strategy = self.strategy if self.strategy else FixedBatchStrategy(1)\n\n    return Dataloader(\n        dataset=dataset,\n        strategy=strategy,\n        batcher=self.batcher,\n        num_workers=self.num_workers or 1,\n        prefetch_factor=self.prefetch_factor or 1,\n        sharding_strategy=self.sharding_strategy,\n        shard_id=self.shard_id,\n        num_shards=self.num_shards,\n    )\n</code></pre>"},{"location":"api/loader/","title":"Dataloader","text":"<p>The Dataloader is the main interface for loading data into your training loop. The Dataloader is responsible for defining how to efficiently load data from a dataset and allocate it to the appropriate devices, batches, and all of the other features that make up proper data loading.</p> <p>The Dataloader works by spawning background workers to prefetch data into a cache, and then filling batches from the cache as they become available. The use of background workers allows the dataloader to be highly efficient and not block the main thread, which is important for training loops. Loadax takes care of the parllelization details for you, so your dataloading is fast, reliable, and simple. The background cache will load out of order, as utilizes mutlithreading to load data in parallel, however the actual batches will be in order. This is because loadax ensures deterministic ordering of batches, and the background workers will load batches in the order that they are requested.</p> Creating a dataloader<pre><code>from loadax import Dataloader, InMemoryDataset, Batcher\nfrom loadax.strategy import FixedBatchStrategy\nfrom loadax.dataloader.sharding import NoShardingStrategy\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = Dataloader(\n    dataset=dataset,\n    batcher=batcher,\n    strategy=FixedBatchStrategy(batch_size=2),\n    num_workers=2,\n    prefetch_factor=2,\n    sharding_strategy=NoShardingStrategy(),\n)\nfor batch in dataloader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>               Bases: <code>NaiveDataloader[DatasetItem, Batch]</code></p> <p>Dataloader that leverages threading for non-blocking data loading.</p> Example <pre><code>from loadax import Dataloader, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = Dataloader(\n    dataset=dataset,\n    batcher=batcher,\n    strategy=FixedBatchStrategy(batch_size=2),\n    num_workers=2,\n    prefetch_factor=2,\n    sharding_strategy=NoShardingStrategy(),\n)\nfor batch in dataloader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to load data from.</p> required <code>batcher</code> <code>Batcher</code> <p>The batcher to use for batching.</p> required <code>strategy</code> <code>BatchStrategy</code> <p>The batch strategy to use for prefetching.</p> required <code>num_workers</code> <code>int</code> <p>The number of workers to use for parallel data loading.</p> required <code>prefetch_factor</code> <code>int</code> <p>The prefetch factor to use for prefetching.</p> required <code>sharding_strategy</code> <code>JaxShardingStrategy</code> <p>The sharding strategy for distributed data loading.</p> required <code>shard_id</code> <code>int | None</code> <p>The ID of this shard (node). If None, then the shard_id will be determined based on the current process index and the number of shards.</p> <code>None</code> <code>num_shards</code> <code>int | None</code> <p>The total number of shards (nodes). If None, then the number of shards will be determined based on the number of processes.</p> <code>None</code> Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset[DatasetItem],\n    batcher: Batcher[DatasetItem, Batch],\n    strategy: BatchStrategy[DatasetItem],\n    num_workers: int,\n    prefetch_factor: int,\n    sharding_strategy: ShardingStrategy,\n    shard_id: int | None = None,\n    num_shards: int | None = None,\n):\n    \"\"\"A dataloader that leverages threading for non-blocking data loading.\n\n    Example:\n        ```python\n        from loadax import Dataloader, InMemoryDataset, Batcher\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        batcher = Batcher(lambda x: x)\n        dataloader = Dataloader(\n            dataset=dataset,\n            batcher=batcher,\n            strategy=FixedBatchStrategy(batch_size=2),\n            num_workers=2,\n            prefetch_factor=2,\n            sharding_strategy=NoShardingStrategy(),\n        )\n        for batch in dataloader:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to load data from.\n        batcher (Batcher): The batcher to use for batching.\n        strategy (BatchStrategy): The batch strategy to use for prefetching.\n        num_workers (int): The number of workers to use for parallel data loading.\n        prefetch_factor (int): The prefetch factor to use for prefetching.\n        sharding_strategy (JaxShardingStrategy): The sharding strategy for\n            distributed data loading.\n        shard_id (int | None): The ID of this shard (node).\n            If None, then the shard_id will be determined based on the\n            current process index and the number of shards.\n        num_shards (int | None): The total number of shards (nodes).\n            If None, then the number of shards will be determined based on\n            the number of processes.\n    \"\"\"\n    super().__init__(dataset, batcher, strategy)\n    assert num_workers &gt; 0, \"num_workers must be at least 1\"\n    self.num_workers = num_workers\n\n    assert prefetch_factor &gt; 0, \"prefetch_factor must be at least 1\"\n    self.prefetch_factor = prefetch_factor\n\n    self.sharding_strategy = sharding_strategy\n\n    # Check that both shard_id and num_shards are provided or neither are provided\n\n    assert (shard_id is None and num_shards is None) or (\n        shard_id is not None and num_shards is not None\n    ), \"Either both shard_id and num_shards must be provided or neither\"\n\n    assert num_shards is None or num_shards &gt; 0, \"num_shards must be greater than 0\"\n    self.num_shards = num_shards or jax.process_count()\n\n    self.shard_id = shard_id or (jax.process_index() % self.num_shards)\n    assert (\n        self.shard_id &lt; self.num_shards\n    ), f\"shard_id {self.shard_id} must be in the range [0, {self.num_shards})\"\n\n    # Determine the shard indices for this node\n    self.shard_indices = self.sharding_strategy.get_shard_indices(\n        dataset_size=len(dataset),\n        shard_id=self.shard_id,\n        num_shards=self.num_shards,\n    )\n</code></pre> <p>               Bases: <code>DataloaderIteratorProtocol[DatasetItem, Batch]</code></p> <p>Iterator for the threaded dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>Dataloader</code> <p>The dataloader to iterate over.</p> required Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def __init__(self, dataloader: \"Dataloader[DatasetItem, Batch]\"):\n    \"\"\"Iterator for the dataloader.\n\n    Args:\n        dataloader (Dataloader): The dataloader to iterate over.\n    \"\"\"\n    self.dataloader = dataloader\n    self.executor = ThreadPoolExecutor(max_workers=self.dataloader.num_workers)\n    self.current_index = 0\n    self.cache: dict[int, DatasetItem] = {}\n    self.futures: dict[int, Future[tuple[int, DatasetItem | None]]] = {}\n    self.cache_lock = threading.Lock()\n    self.index_lock = threading.Lock()\n\n    self.shard_indices = list(self.dataloader.shard_indices)\n    self.total_indices = len(self.shard_indices)\n\n    self._shutdown_finalizer = weakref.finalize(self, self._shutdown_executor)\n\n    self._start_prefetching()\n</code></pre>"},{"location":"api/loader/#loadax.dataloader.loader.DataloaderIterator.progress","title":"progress","text":"<pre><code>progress() -&gt; Progress\n</code></pre> <p>Get the progress of the dataloader.</p> Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def progress(self) -&gt; Progress:\n    \"\"\"Get the progress of the dataloader.\"\"\"\n    return Progress(self.current_index, len(self.dataloader.shard_indices))\n</code></pre>"},{"location":"api/naive/","title":"Naive Dataloader","text":"<p>The naive dataloader is the simplest form of a dataloader. Do not use this dataloader unless you have a specific reason to do so. It is fine for a quick experiment or for debuugging, but the naive dataloader can block your training loop, does not have sharding support, and is not recommended for production use. As a result, if you use the <code>DataLoader</code> builder to create your dataloader, the naive dataloader will never be used.</p> <p>               Bases: <code>DataloaderProtocol[DatasetItem, Batch]</code>, <code>Generic[DatasetItem, Batch]</code></p> <p>A naive dataloader that does not offload work to background processes.</p> <p>This dataloader is a simple implementation of a dataloader that does not offload work to background processes. It is useful for debugging and testing. This will slow down your training loop, but is useful for debugging and really simple training loops.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The dataset to load.</p> <code>batcher</code> <p>The batcher to use to collate the data.</p> <code>strategy</code> <p>The batch strategy to use to determine how much data goes into a batch.</p> <p>This dataloader is a simple implementation of a dataloader that does not offload work to background processes. It is useful for debugging and testing. This will slow down your training loop, but is useful for debugging and really simple training loops.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[DatasetItem]</code> <p>The dataset to load.</p> required <code>batcher</code> <code>Batcher[DatasetItem, Batch]</code> <p>The batcher to use to collate the data.</p> required <code>strategy</code> <code>BatchStrategy[DatasetItem]</code> <p>The batch strategy to use to determine how much data goes into a batch.</p> required Source code in <code>src/loadax/dataloader/naive.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset[DatasetItem],\n    batcher: Batcher[DatasetItem, Batch],\n    strategy: BatchStrategy[DatasetItem],\n):\n    \"\"\"A naive dataloader that does not offload work to background processes.\n\n    This dataloader is a simple implementation of a dataloader that does not\n    offload work to background processes. It is useful for debugging and testing.\n    This will slow down your training loop, but is useful for debugging and really\n    simple training loops.\n\n    Args:\n        dataset: The dataset to load.\n        batcher: The batcher to use to collate the data.\n        strategy: The batch strategy to use to determine how much data goes into a\n            batch.\n    \"\"\"\n    self.dataset = dataset\n    self.batcher = batcher\n    self.strategy = strategy\n</code></pre>"},{"location":"api/naive/#loadax.dataloader.naive.NaiveDataloader.num_items","title":"num_items","text":"<pre><code>num_items() -&gt; int\n</code></pre> <p>Get the number of items in the dataset.</p> <p>This method returns the number of items in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of items in the dataset.</p> Source code in <code>src/loadax/dataloader/naive.py</code> <pre><code>def num_items(self) -&gt; int:\n    \"\"\"Get the number of items in the dataset.\n\n    This method returns the number of items in the dataset.\n\n    Returns:\n        The number of items in the dataset.\n    \"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/naive/#loadax.dataloader.naive.NaiveDataloader.num_batches","title":"num_batches","text":"<pre><code>num_batches() -&gt; int\n</code></pre> <p>Get the number of batches in the dataset.</p> <p>This method returns the number of batches in the dataset based on your batch strategy.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of batches in the dataset.</p> Source code in <code>src/loadax/dataloader/naive.py</code> <pre><code>def num_batches(self) -&gt; int:\n    \"\"\"Get the number of batches in the dataset.\n\n    This method returns the number of batches in the dataset based\n    on your batch strategy.\n\n    Returns:\n        The number of batches in the dataset.\n    \"\"\"\n    return self.num_items() // self.strategy.batch_size\n</code></pre>"},{"location":"api/progress/","title":"Progress","text":"<p>Progress is a simple interface that allows you to track the progress of a dataloader. Because a dataloader is a stateful iterator, you can use the <code>progress()</code> method to get the current progress of the iterator. This means no more manual calculations of the progress, dealing with batch sizes, or forgetting to update your progress tracking.</p> Tracking progress<pre><code>from loadax import Dataloader, InMemoryDataset, Batcher\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\ndataloader = Dataloader(batcher).batch_size(2).build(dataset)\n\niterator = iter(dataloader)\n\nfor batch in iterator:\n    print(batch)\n    iterator.progress()\n\n#&gt; [1, 2]\n#&gt; Progress(items_processed=2, items_total=5)\n#&gt; [3, 4]\n#&gt; Progress(items_processed=4, items_total=5)\n#&gt; [5]\n#&gt; Progress(items_processed=5, items_total=5)\n</code></pre> <p>Progress metadata for a dataloader.</p> <p>This metadata indicates how far the dataloader has progressed. This is useful for debugging and monitoring the progress of a dataloader.</p> <p>Attributes:</p> Name Type Description <code>items_processed</code> <code>int</code> <p>The number of items processed so far.</p> <code>items_total</code> <code>int</code> <p>The total number of items in the dataset.</p>"},{"location":"api/dataset/dataset/","title":"Dataset","text":"<p>A dataset is a simple interface that defines how to load data from a source. All datasets must implement this interface, and it is the responsibility of the dataset to load the data from the underlying source. Because datasets support random access, they should also know their size.</p> Creating a dataset<pre><code>from loadax import Dataset\n\nclass MyDataset(Dataset[int]):\n    def __init__(self, items: list[int]):\n        self.items = items\n\n    def get(self, index: int) -&gt; int:\n        return self.items[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\ndataset = MyDataset([1, 2, 3, 4, 5])\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[DatasetItem]</code></p> <p>Dataset is a protocol for datasets.</p> <p>Any loadax dataset must implement the Dataset protocol. This protocol defines the basic functionality needed for a random access dataset. This includes the ability to get an item at a given index, as well as the ability to iterate over the dataset.</p> Example <pre><code>from loadax import InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nprint(dataset.get(0))\n\n#&gt; 1\n</code></pre> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>The dataset to iterate over.</p> <code>current</code> <code>int</code> <p>The current index of the iterator.</p> <p>Any loadax dataset must implement the Dataset protocol. This protocol defines thebasic functionality needed for a random access dataset. This includes the ability to get an item at a given index, as well as the ability to iterate over the dataset.</p> Example <pre><code>from loadax import InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nprint(dataset.get(0))\n\n#&gt; 1\n</code></pre> Source code in <code>src/loadax/dataset/protocol.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Dataset is a protocol for datasets.\n\n    Any loadax dataset must implement the Dataset protocol. This protocol defines\n    thebasic functionality needed for a random access dataset. This includes the\n    ability to get an item at a given index, as well as the ability to iterate over\n    the dataset.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        print(dataset.get(0))\n\n        #&gt; 1\n        ```\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/dataset/dataset/#loadax.dataset.protocol.Dataset.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <pre><code>from loadax import InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nprint(dataset.get(0))\n\n#&gt; 1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/protocol.py</code> <pre><code>@abstractmethod\ndef get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        print(dataset.get(0))\n\n        #&gt; 1\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/dataset/dataset/#loadax.dataset.protocol.Dataset.is_empty","title":"is_empty","text":"<pre><code>is_empty() -&gt; bool\n</code></pre> <p>Check if the dataset is empty.</p> <p>This method returns True if the dataset is empty, and False otherwise.</p> Example <pre><code>from loadax import InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nprint(dataset.is_empty())\n\n#&gt; False\n</code></pre> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the dataset is empty, and False otherwise.</p> Source code in <code>src/loadax/dataset/protocol.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the dataset is empty.\n\n    This method returns True if the dataset is empty, and False otherwise.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        print(dataset.is_empty())\n\n        #&gt; False\n        ```\n\n    Returns:\n        bool: True if the dataset is empty, and False otherwise.\n    \"\"\"\n    return len(self) == 0\n</code></pre>"},{"location":"api/dataset/in-memory/","title":"InMemoryDataset","text":"<p>The InMemoryDataset is a simple dataset that stores all underlying items in a list in memory. This is a simple dataset and is only useful for small datasets and debugging.</p> Creating an in-memory dataset<pre><code>from loadax import InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>InMemoryDataset is a simple in-memory dataset.</p> <p>This dataset stores all underlying items in a list in memory. This is a simple implementation, in your training loop if you are using non-trivial dataset it is better to use another dataset. This dataset type does not get much benefit from dataloaders, but it is useful for debugging and for simple use cases.</p> Example <p>dataset = InMemoryDataset([1, 2, 3, 4, 5]) print(dataset.get(0))</p> <p>Attributes:</p> Name Type Description <code>items</code> <code>list[DatasetItem]</code> <p>The underlying items in the dataset.</p> <p>This dataset stores all underlying items in a list in memory. This is a simple implementation, in your training loop if you are using non-trivial dataset it is better to use another dataset. This dataset type does not get much benefit from dataloaders, but it is useful for debugging and for simple use cases.</p> Example <p>dataset = InMemoryDataset([1, 2, 3, 4, 5]) print(dataset.get(0))</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[DatasetItem]</code> <p>The underlying items in the dataset.</p> required Source code in <code>src/loadax/dataset/in_memory.py</code> <pre><code>def __init__(self, items: list[DatasetItem]):\n    \"\"\"InMemoryDataset is a simple in-memory dataset.\n\n    This dataset stores all underlying items in a list in memory. This is a simple\n    implementation, in your training loop if you are using non-trivial dataset\n    it is better to use another dataset. This dataset type does not get much benefit\n    from dataloaders, but it is useful for debugging and for simple use cases.\n\n    Example:\n        &gt;&gt;&gt; dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; print(dataset.get(0))\n\n    Args:\n        items (list[DatasetItem]): The underlying items in the dataset.\n    \"\"\"\n    self.items = items\n</code></pre>"},{"location":"api/dataset/in-memory/#loadax.dataset.in_memory.InMemoryDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <p>dataset = InMemoryDataset([1, 2, 3, 4, 5]) print(dataset.get(0))</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/in_memory.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        &gt;&gt;&gt; dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; print(dataset.get(0))\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    if index &gt;= len(self.items):\n        return None\n\n    if index &lt; 0:\n        index = len(self.items) + index\n    return self.items[index]\n</code></pre>"},{"location":"api/dataset/range/","title":"RangeDataset","text":"<p>The RangeDataset is a simple dataset that returns a range of integers. This is useful for testing and debugging.</p> Creating a range dataset<pre><code>from loadax import RangeDataset\n\ndataset = RangeDataset(start=0, end=10, step=2)\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 0\n#&gt; 2\n#&gt; 4\n#&gt; 6\n#&gt; 8\n</code></pre> <p>               Bases: <code>Dataset[int]</code></p> <p>Dataset that returns a range of integers.</p> <p>This dataset is useful for testing and debugging.</p> <p>Attributes:</p> Name Type Description <code>start</code> <p>The start of the range.</p> <code>end</code> <p>The end of the range.</p> <code>step</code> <p>The step size of the range.</p> <p>This dataset is useful for testing and debugging.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>The start of the range.</p> required <code>end</code> <code>int</code> <p>The end of the range.</p> required <code>step</code> <code>int</code> <p>The step size of the range.</p> <code>1</code> Source code in <code>src/loadax/dataset/range.py</code> <pre><code>def __init__(self, start: int, end: int, step: int = 1):\n    \"\"\"Dataset that returns a range of integers.\n\n    This dataset is useful for testing and debugging.\n\n    Args:\n        start: The start of the range.\n        end: The end of the range.\n        step: The step size of the range.\n    \"\"\"\n    self.start = start\n    self.end = end\n    self.step = step\n\n    if self.start &gt;= self.end:\n        raise ValueError(\"start must be less than end\")\n    if self.step &lt;= 0:\n        raise ValueError(\"step must be greater than 0\")\n</code></pre>"},{"location":"api/dataset/range/#loadax.dataset.range.RangeDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; int | None\n</code></pre> <p>Get the item at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>The item at the given index, or None if the index is out of range.</p> Source code in <code>src/loadax/dataset/range.py</code> <pre><code>def get(self, index: int) -&gt; int | None:\n    \"\"\"Get the item at the given index.\n\n    Args:\n        index: The index of the item to get.\n\n    Returns:\n        The item at the given index, or None if the index is out of range.\n    \"\"\"\n    if index &lt; 0:\n        index = len(self) + index\n\n    if 0 &lt;= index &lt; len(self):\n        return self.start + index * self.step\n\n    return None\n</code></pre>"},{"location":"api/dataset/transformations/combined/","title":"CombinedDataset","text":"<p>A combined dataset is a simple dataset that combines multiple underlying datasets. The combined dataset will return the items from the first underlying dataset, then the items from the second underlying dataset, and so on.</p> Creating a combined dataset<pre><code>from loadax import CombinedDataset, InMemoryDataset\n\ndataset1 = InMemoryDataset([1, 2, 3, 4, 5])\ndataset2 = InMemoryDataset([6, 7, 8, 9, 10])\ncombined_dataset = CombinedDataset(dataset1, dataset2)\n\nfor i in range(len(combined_dataset)):\n    print(combined_dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n#&gt; 6\n#&gt; 7\n#&gt; 8\n#&gt; 9\n#&gt; 10\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>Combine multiple datasets into a single dataset.</p> <p>This is a simple concatenation of the datasets. The datasets are concatenated in the order they are passed to the constructor. If the dataset elements need to be interleaved, you can either use a <code>ShuffledDataset</code>, <code>SampledDatasetWithoutReplacement</code> or <code>SampledDatasetWithReplacement</code>.</p> Example <pre><code>from loadax import CombinedDataset, InMemoryDataset\n\ndataset1 = InMemoryDataset([1, 2, 3])\ndataset2 = InMemoryDataset([4, 5, 6])\ncombined_dataset = CombinedDataset([dataset1, dataset2])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>list[Dataset]</code> <p>The datasets to combine.</p> required Source code in <code>src/loadax/dataset/transform/combined.py</code> <pre><code>def __init__(self, datasets: list[Dataset[DatasetItem]]):\n    \"\"\"Combine multiple datasets into a single dataset.\n\n    This is a simple concatenation of the datasets. The datasets are\n    concatenated in the order they are passed to the constructor. If the\n    dataset elements need to be interleaved, you can either use a `ShuffledDataset`,\n    `SampledDatasetWithoutReplacement` or `SampledDatasetWithReplacement`.\n\n    Example:\n        ```python\n        from loadax import CombinedDataset, InMemoryDataset\n\n        dataset1 = InMemoryDataset([1, 2, 3])\n        dataset2 = InMemoryDataset([4, 5, 6])\n        combined_dataset = CombinedDataset([dataset1, dataset2])\n        ```\n\n    Args:\n        datasets (list[Dataset]): The datasets to combine.\n    \"\"\"\n    self.datasets = datasets\n</code></pre>"},{"location":"api/dataset/transformations/combined/#loadax.dataset.transform.CombinedDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> Example <pre><code>from loadax import CombinedDataset, InMemoryDataset\n\ndataset1 = InMemoryDataset([1, 2, 3])\ndataset2 = InMemoryDataset([4, 5, 6])\ncombined_dataset = CombinedDataset([dataset1, dataset2])\nprint(combined_dataset.get(0))\n#&gt; 1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/transform/combined.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    Example:\n        ```python\n        from loadax import CombinedDataset, InMemoryDataset\n\n        dataset1 = InMemoryDataset([1, 2, 3])\n        dataset2 = InMemoryDataset([4, 5, 6])\n        combined_dataset = CombinedDataset([dataset1, dataset2])\n        print(combined_dataset.get(0))\n        #&gt; 1\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    current_index = 0\n    for dataset in self.datasets:\n        if index &lt; current_index + len(dataset):\n            return dataset.get(index - current_index)\n        current_index += len(dataset)\n    return None\n</code></pre>"},{"location":"api/dataset/transformations/mapped/","title":"MappedDataset","text":"<p>A mapped dataset is a transformation that applies a functional transformation to the underlying dataset. The transformation is applied lazily, the underlying dataset is preserved. Because loadax performs dataloading in the background, this means it is acceptable to perform lightweight data augmentation or transformations on the dataset.</p> <p>If you have some complicated transformations you may still want to perform them ahead of time.</p> Creating a mapped dataset<pre><code>from loadax import MappedDataset, InMemoryDataset\n\ndef transform(x):\n    return x * 2\n\ndataset = MappedDataset(InMemoryDataset([1, 2, 3, 4, 5]), transform)\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 2\n#&gt; 4\n#&gt; 6\n#&gt; 8\n#&gt; 10\n</code></pre> <p>               Bases: <code>Dataset[MappedItem]</code></p> <p>Dataset that applies a function to the items of another dataset.</p> <p>This dataset transformation lazily applies a function to the items of another dataset. This is useful for cases where you need to augment the data before using it in a training loop.</p> Example <pre><code>from loadax.dataset import RangeDataset\nfrom loadax.dataset.transform import MappedDataset\n\ndef slow_fn(x):\n    time.sleep(0.1)\n    return x * 2\n\ndataset = MappedDataset(RangeDataset(0, 20), slow_fn)\n</code></pre> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The dataset to apply the function to.</p> <code>map_fn</code> <p>The function to apply to the items of the dataset.</p> <p>This dataset transformation lazily applies a function to the items of another dataset. This is useful for cases where you need to augment the data before usingit in a training loop.</p> Example <pre><code>from loadax.dataset import RangeDataset\nfrom loadax.dataset.transform import MappedDataset\n\ndef slow_fn(x):\n    time.sleep(0.1)\n    return x * 2\n\ndataset = MappedDataset(RangeDataset(0, 20), slow_fn)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[DatasetItem]</code> <p>The dataset to apply the function to.</p> required <code>map_fn</code> <code>Callable[[DatasetItem], MappedItem]</code> <p>The function to apply to the items of the dataset.</p> required Source code in <code>src/loadax/dataset/transform/mapped.py</code> <pre><code>def __init__(\n    self, dataset: Dataset[DatasetItem], map_fn: Callable[[DatasetItem], MappedItem]\n):\n    \"\"\"Dataset that applies a function to the items of another dataset.\n\n    This dataset transformation lazily applies a function to the items of another\n    dataset. This is useful for cases where you need to augment the data before\n    usingit in a training loop.\n\n    Example:\n        ```python\n        from loadax.dataset import RangeDataset\n        from loadax.dataset.transform import MappedDataset\n\n        def slow_fn(x):\n            time.sleep(0.1)\n            return x * 2\n\n        dataset = MappedDataset(RangeDataset(0, 20), slow_fn)\n        ```\n\n    Args:\n        dataset: The dataset to apply the function to.\n        map_fn: The function to apply to the items of the dataset.\n    \"\"\"\n    self.dataset = dataset\n    self.map_fn = map_fn\n</code></pre>"},{"location":"api/dataset/transformations/mapped/#loadax.dataset.transform.MappedDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; MappedItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>MappedItem | None</code> <p>The item at the given index, or None if the index is out of range.</p> Source code in <code>src/loadax/dataset/transform/mapped.py</code> <pre><code>def get(self, index: int) -&gt; MappedItem | None:\n    \"\"\"Get the item at the given index.\n\n    Args:\n        index: The index of the item to get.\n\n    Returns:\n        The item at the given index, or None if the index is out of range.\n    \"\"\"\n    item = self.dataset.get(index)\n    if item is None:\n        return None\n    return self.map_fn(item)\n</code></pre>"},{"location":"api/dataset/transformations/partial/","title":"PartialDataset","text":"<p>A partial dataset is a simple dataset that returns a subset of the underlying dataset. This is useful for testing and debugging.</p> Creating a partial dataset<pre><code>from loadax import PartialDataset, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\npartial_dataset = PartialDataset(dataset, 2, 4)\n\nfor i in range(len(partial_dataset)):\n    print(partial_dataset.get(i))\n\n#&gt; 2\n#&gt; 3\n#&gt; 4\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>Create a partial subset of an existing dataset.</p> <p>This dataset allows accessing a subset of the original dataset. The subset is defined by the start and end indices.</p> Example <pre><code>from loadax import InMemoryDataset, PartialDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\npartial_dataset = PartialDataset(dataset, 1, 4)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to create the partial subset from.</p> required <code>start_index</code> <code>int</code> <p>The start index of the subset.</p> required <code>end_index</code> <code>int</code> <p>The end index of the subset.</p> required Source code in <code>src/loadax/dataset/transform/partial.py</code> <pre><code>def __init__(\n    self, dataset: Dataset[DatasetItem], start_index: int, end_index: int\n) -&gt; None:\n    \"\"\"Create a partial subset of an existing dataset.\n\n    This dataset allows accessing a subset of the original dataset. The\n    subset is defined by the start and end indices.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset, PartialDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        partial_dataset = PartialDataset(dataset, 1, 4)\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to create the partial subset from.\n        start_index (int): The start index of the subset.\n        end_index (int): The end index of the subset.\n    \"\"\"\n    self.dataset = dataset\n    self.start_index = start_index\n    self.end_index = min(end_index, len(dataset))\n</code></pre>"},{"location":"api/dataset/transformations/partial/#loadax.dataset.transform.PartialDataset.split","title":"split  <code>staticmethod</code>","text":"<pre><code>split(dataset: Dataset[DatasetItem], num_parts: int) -&gt; Sequence[Dataset[DatasetItem]]\n</code></pre> <p>Split a dataset into multiple parts.</p> <p>This method splits a dataset into multiple parts of equal size. The number of parts is determined by the <code>num_parts</code> argument. The last part may contain fewer items if the dataset is not evenly divisible by the number of parts.</p> Example <pre><code>from loadax import InMemoryDataset, PartialDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\npartial_datasets = PartialDataset.split(dataset, 3)\n\nfor partial_dataset in partial_datasets:\n    print(partial_dataset.get(0))\n\n#&gt; 1\n#&gt; 3\n#&gt; 5\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to split.</p> required <code>num_parts</code> <code>int</code> <p>The number of parts to split the dataset into.</p> required <p>Returns:</p> Type Description <code>Sequence[Dataset[DatasetItem]]</code> <p>list[Dataset[DatasetItem]]: A list of partial datasets.</p> Source code in <code>src/loadax/dataset/transform/partial.py</code> <pre><code>@staticmethod\ndef split(\n    dataset: Dataset[DatasetItem], num_parts: int\n) -&gt; Sequence[Dataset[DatasetItem]]:\n    \"\"\"Split a dataset into multiple parts.\n\n    This method splits a dataset into multiple parts of equal size. The\n    number of parts is determined by the `num_parts` argument. The last\n    part may contain fewer items if the dataset is not evenly divisible\n    by the number of parts.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset, PartialDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        partial_datasets = PartialDataset.split(dataset, 3)\n\n        for partial_dataset in partial_datasets:\n            print(partial_dataset.get(0))\n\n        #&gt; 1\n        #&gt; 3\n        #&gt; 5\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to split.\n        num_parts (int): The number of parts to split the dataset into.\n\n    Returns:\n        list[Dataset[DatasetItem]]: A list of partial datasets.\n    \"\"\"\n    total_len = len(dataset)\n    batch_size = total_len // num_parts\n    remainder = total_len % num_parts\n    datasets = []\n\n    current = 0\n    for i in range(num_parts):\n        start = current\n        end = start + batch_size + (1 if i &lt; remainder else 0)\n        datasets.append(PartialDataset(dataset, start, end))\n        current = end\n\n    return datasets\n</code></pre>"},{"location":"api/dataset/transformations/partial/#loadax.dataset.transform.PartialDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <pre><code>from loadax import InMemoryDataset, PartialDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\npartial_dataset = PartialDataset(dataset, 1, 4)\nprint(partial_dataset.get(0))\n\n#&gt; 2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/transform/partial.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        ```python\n        from loadax import InMemoryDataset, PartialDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        partial_dataset = PartialDataset(dataset, 1, 4)\n        print(partial_dataset.get(0))\n\n        #&gt; 2\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    if index &lt; 0:\n        index += len(self)\n    if index &lt; 0 or index &gt;= len(self):\n        return None\n    return self.dataset.get(index + self.start_index)\n</code></pre>"},{"location":"api/dataset/transformations/sampled/","title":"SampledDataset","text":"<p>A sampled dataset is a simple dataset that returns a subset of the underlying dataset. The sampling is performed lazily and does not actually sample the underlying storage. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The sampling procedure is deterministic and leverages JAX's random number generation.</p> Creating a sampled dataset<pre><code>from loadax import SampledDatasetWithReplacement, InMemoryDataset\nimport jax\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>Sample a subset of the items in the source dataset without replacement.</p> <p>This dataset allows sampling a subset of the items in the source dataset without replacement. The sampling eagerly shuffles the indices of the source dataset and then selects the first <code>sample_size</code> indices. This is a simple implementation that does not perform any kind of sampling with replacement.</p> <p>The sampling does not alter the underlying storage of the source dataset. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The lazy sampling enables better reproducibility and determinism, while also being extremely efficient as only the indices need to be sampled. This also means that sampling is not IO bound as the indices can be stored in memory.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithoutReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithoutReplacement(dataset, 3, key)\n</code></pre> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>The underlying dataset to sample from.</p> <code>sample_size</code> <code>int</code> <p>The size of the sample to take.</p> <code>indices</code> <code>list[int]</code> <p>The indices to sample from.</p> <p>This dataset allows sampling a subset of the items in the source dataset without replacement. The sampling eagerly shuffles the indices of the source dataset and then selects the first <code>sample_size</code> indices. This is a simple implementation that does not perform any kind of sampling with replacement.</p> <p>The sampling does not alter the underlying storage of the source dataset. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases,this should not be necessary.</p> <p>The lazy sampling enables better reproducibility and determinism, while also being extremely efficient as only the indices need to be sampled. This also means that sampling is not IO bound as the indices can be stored in memory.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithoutReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithoutReplacement(dataset, 3, key)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to sample from.</p> required <code>sample_size</code> <code>int</code> <p>The size of the sample to take.</p> required <code>key</code> <code>KeyArray</code> <p>The key to use for sampling.</p> required Source code in <code>src/loadax/dataset/transform/sampled.py</code> <pre><code>def __init__(self, dataset: Dataset[DatasetItem], sample_size: int, key: jax.Array):\n    \"\"\"Sample a subset of the items in the source dataset without replacement.\n\n    This dataset allows sampling a subset of the items in the source dataset without\n    replacement. The sampling eagerly shuffles the indices of the source dataset and\n    then selects the first `sample_size` indices. This is a simple implementation\n    that does not perform any kind of sampling with replacement.\n\n    The sampling does not alter the underlying storage of the source dataset. If\n    your underlying storage does not perform well with random access, you may want\n    to consider performing the sampling in advance. However, for almost all use\n    cases,this should not be necessary.\n\n    The lazy sampling enables better reproducibility and determinism, while also\n    being extremely efficient as only the indices need to be sampled. This also\n    means that sampling is not IO bound as the indices can be stored in memory.\n\n    Example:\n        ```python\n        import jax\n        from loadax import SampledDatasetWithoutReplacement, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        sampled_dataset = SampledDatasetWithoutReplacement(dataset, 3, key)\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to sample from.\n        sample_size (int): The size of the sample to take.\n        key (jax.random.KeyArray): The key to use for sampling.\n    \"\"\"\n    self.dataset = dataset\n    self.sample_size = sample_size\n    self.indices: list[int] = []\n    self.key = key\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>Sample a subset of the items in the source dataset with replacement.</p> <p>This dataset allows sampling a subset of the items in the source dataset with replacement. The sampling lazily selects random indices from the source dataset.</p> <p>The sampling does not alter the underlying storage of the source dataset. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases, this should not be necessary.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\n</code></pre> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>The underlying dataset to sample from.</p> <code>sample_size</code> <code>int</code> <p>The size of the sample to take.</p> <p>This dataset allows sampling a subset of the items in the source dataset with replacement. The sampling lazily selects random indices from the source dataset.</p> <p>The sampling does not alter the underlying storage of the source dataset. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases, this should not be necessary.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to sample from.</p> required <code>sample_size</code> <code>int</code> <p>The size of the sample to take.</p> required <code>key</code> <code>KeyArray</code> <p>The key to use for sampling.</p> required Source code in <code>src/loadax/dataset/transform/sampled.py</code> <pre><code>def __init__(self, dataset: Dataset[DatasetItem], sample_size: int, key: jax.Array):\n    \"\"\"Sample a subset of the items in the source dataset with replacement.\n\n    This dataset allows sampling a subset of the items in the source dataset with\n    replacement. The sampling lazily selects random indices from the source dataset.\n\n    The sampling does not alter the underlying storage of the source dataset. If\n    your underlying storage does not perform well with random access, you may want\n    to consider performing the sampling in advance. However, for almost all use\n    cases, this should not be necessary.\n\n    Example:\n        ```python\n        import jax\n        from loadax import SampledDatasetWithReplacement, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        sampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to sample from.\n        sample_size (int): The size of the sample to take.\n        key (jax.random.KeyArray): The key to use for sampling.\n    \"\"\"\n    self.dataset = dataset\n    self.sample_size = sample_size\n    self.key = key\n</code></pre>"},{"location":"api/dataset/transformations/sampled/#loadax.dataset.transform.SampledDatasetWithoutReplacement.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithoutReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithoutReplacement(dataset, 3, key)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/transform/sampled.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        ```python\n        import jax\n        from loadax import SampledDatasetWithoutReplacement, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        sampled_dataset = SampledDatasetWithoutReplacement(dataset, 3, key)\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    if index &gt;= self.sample_size or len(self.dataset) == 0:\n        return None\n    return self.dataset.get(self._index())\n</code></pre>"},{"location":"api/dataset/transformations/sampled/#loadax.dataset.transform.SampledDatasetWithReplacement.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <pre><code>import jax\nfrom loadax import SampledDatasetWithReplacement, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\nprint(sampled_dataset.get(0))\n#&gt; 1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/transform/sampled.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        ```python\n        import jax\n        from loadax import SampledDatasetWithReplacement, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        sampled_dataset = SampledDatasetWithReplacement(dataset, 3, key)\n        print(sampled_dataset.get(0))\n        #&gt; 1\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    if index &gt;= self.sample_size or len(self.dataset) == 0:\n        return None\n\n    self.key, subkey = jax.random.split(self.key)\n    random_index = jax.random.randint(subkey, (), 0, len(self.dataset))\n    return self.dataset.get(int(random_index))\n</code></pre>"},{"location":"api/dataset/transformations/shuffled/","title":"ShuffledDataset","text":"<p>A shuffled dataset is a simple dataset that shuffles the items in the underlying dataset. The shuffling is performed lazily and does not actually shuffle the underlying storage. If your underlying storage does not perform well with random access, you may want to consider performing the shuffling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The shuffling procedure is deterministic and leverages JAX's random number generation.</p> Creating a shuffled dataset<pre><code>from loadax import ShuffledDataset, InMemoryDataset\nimport jax\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nshuffled_dataset = ShuffledDataset(dataset, key)\n</code></pre> <p>               Bases: <code>Dataset[DatasetItem]</code></p> <p>Shuffle the items in the source dataset.</p> <p>The shuffling is performed lazily and does not actually shuffle the underlying storage. If your underlying storage does not perform well with random access, you may want to consider performing the shuffling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The lazy shuffling enables  better reproducibility and determinism, while also being extremely efficient as only the indices need to be shuffled. This also means that shuffling is not IO bound as the indices can be stored in memory.</p> Example <pre><code>import jax\nfrom loadax import ShuffledDataset, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nshuffled_dataset = ShuffledDataset(dataset, key)\n</code></pre> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>The underlying dataset to shuffle.</p> <code>indices</code> <code>list[int]</code> <p>The indices to shuffle.</p> <p>The shuffling is performed lazily and does not actually shuffle the underlying storage. If your underlying storage does not perform well with random access, you may want to consider performing the shuffling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The lazy shuffling enables better reproducibility and determinism, while also being extremely efficient as only the indices need to be shuffled. This also means that shuffling is not IO bound as the indices can be stored in memory.</p> Example <pre><code>import jax\nfrom loadax import ShuffledDataset, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nshuffled_dataset = ShuffledDataset(dataset, key)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to shuffle.</p> required <code>key</code> <code>KeyArray</code> <p>The key to use for shuffling.</p> required Source code in <code>src/loadax/dataset/transform/shuffled.py</code> <pre><code>def __init__(self, dataset: Dataset[DatasetItem], key: jax.Array):\n    \"\"\"Shuffle the items in the source dataset.\n\n    The shuffling is performed lazily and does not actually shuffle the underlying\n    storage. If your underlying storage does not perform well with random access,\n    you may want to consider performing the shuffling in advance. However, for\n    almost all use cases, this should not be necessary.\n\n    The lazy shuffling enables better reproducibility and determinism, while also\n    being extremely efficient as only the indices need to be shuffled. This also\n    means that shuffling is not IO bound as the indices can be stored in memory.\n\n    Example:\n        ```python\n        import jax\n        from loadax import ShuffledDataset, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        shuffled_dataset = ShuffledDataset(dataset, key)\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to shuffle.\n        key (jax.random.KeyArray): The key to use for shuffling.\n    \"\"\"\n    self.dataset = dataset\n    self.key = key\n    self.indices = list(jax.random.permutation(self.key, len(dataset)))\n</code></pre>"},{"location":"api/dataset/transformations/shuffled/#loadax.dataset.transform.ShuffledDataset.get","title":"get","text":"<pre><code>get(index: int) -&gt; DatasetItem | None\n</code></pre> <p>Get the item at the given index.</p> <p>This method returns the item at the given index in the dataset. If the index is negative, it respects python's negative indexing semantics. If the index is out of bounds, it returns None.</p> Example <pre><code>import jax\nfrom loadax import ShuffledDataset, InMemoryDataset\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nshuffled_dataset = ShuffledDataset(dataset, key)\nprint(shuffled_dataset.get(0))\n#&gt; 3\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Type Description <code>DatasetItem | None</code> <p>DatasetItem | None: The item at the given index, or None if the index is out of bounds.</p> Source code in <code>src/loadax/dataset/transform/shuffled.py</code> <pre><code>def get(self, index: int) -&gt; DatasetItem | None:\n    \"\"\"Get the item at the given index.\n\n    This method returns the item at the given index in the dataset. If the\n    index is negative, it respects python's negative indexing semantics.\n    If the index is out of bounds, it returns None.\n\n    Example:\n        ```python\n        import jax\n        from loadax import ShuffledDataset, InMemoryDataset\n\n        dataset = InMemoryDataset([1, 2, 3, 4, 5])\n        key = jax.random.PRNGKey(0)\n        shuffled_dataset = ShuffledDataset(dataset, key)\n        print(shuffled_dataset.get(0))\n        #&gt; 3\n        ```\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        DatasetItem | None: The item at the given index, or None if the index is\n            out of bounds.\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self.indices):\n        return None\n    return self.dataset.get(self.indices[index])\n</code></pre>"},{"location":"api/sharding/presets/","title":"Sharding Presets","text":"<p>Loadax provides a few common sharding configurations that can be used out of the box. These presets are strong starting points for your sharding configuration, but you can also create your own sharding configurations using JAX's <code>Mesh</code> and <code>NamedSharding</code> primitives.</p>"},{"location":"api/sharding/presets/#fsdp-sharding","title":"FSDP Sharding","text":"<p>The FSDP sharding preset is a simple configuration that shards the model across multiple hosts and devices, and the data across multiple hosts. In FSDP training, the model parameters are split across multiple devices and each model shard recieves unique data. This configuration is useful for training models that are too large to fit on a single device.</p> Creating an FSDP sharding preset<pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\nfrom loadax.sharding_utilities import fsdp_sharding\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\n\nmesh, axis_names = fsdp_sharding()\n\ndataloader = DataloaderBuilder(batcher)\n    .batch_size(2)\n    .shard(mesh, axis_names[0])\n    .build(dataset)\n</code></pre> <p>Default sharding configuration for FSDP.</p> <p>FSDP is a distributed training technique where both the model's parameters and the data are sharded across multipled devices and potentially multiple nodes. This configuration ensures that each node manages a subset of the data and holds a partition of the model's parameters. By sharding the model, FSDP enables training of models that are too large to fit on a single device, optimize memory usage and computational efficiency.</p> <p>Loadax's default FSDP sharding configuration leverages JAX's parallelization primitives to minimize network traffic and optimize data loading. The model's parameters are distributed across devices within each data shard, allowing for scalable training across different architectures and network topologies. The data is partitioned such that each node can precompute what data it needs to load, reducing network traffic and improving data loading efficiency.</p> <p>In the case of single-node training, this configuration is still valid and only utilizes a single data shard. The data is still able to be used across all local devices, this is just to mean that the sharding splits are not pre-computed.</p> <p>This configuration is useful for FSDP training, but you can also create your own sharding configurations. See the documentation for more information.</p> <p>Returns:</p> Name Type Description <code>mesh</code> <code>Mesh</code> <p>The mesh to use for sharding.</p> <code>axis_names</code> <code>tuple[str]</code> <p>The axis names to use for sharding. The first axis name is used for the data sharding, and the second axis name is used for the model sharding.</p> Source code in <code>src/loadax/sharding_utilities.py</code> <pre><code>def fsdp_sharding() -&gt; ShardingPreset:\n    \"\"\"Default sharding configuration for FSDP.\n\n    FSDP is a distributed training technique where both the model's parameters and the\n    data are sharded across multipled devices and potentially multiple nodes. This\n    configuration ensures that each node manages a subset of the data and holds a\n    partition of the model's parameters. By sharding the model, FSDP enables training\n    of models that are too large to fit on a single device, optimize memory usage and\n    computational efficiency.\n\n    Loadax's default FSDP sharding configuration leverages JAX's parallelization\n    primitives to minimize network traffic and optimize data loading. The model's\n    parameters are distributed across devices within each data shard, allowing for\n    scalable training across different architectures and network topologies. The data\n    is partitioned such that each node can precompute what data it needs to load,\n    reducing network traffic and improving data loading efficiency.\n\n    In the case of single-node training, this configuration is still valid and only\n    utilizes a single data shard. The data is still able to be used across all local\n    devices, this is just to mean that the sharding splits are not pre-computed.\n\n    This configuration is useful for FSDP training, but you can also create your own\n    sharding configurations. See the documentation for more information.\n\n    Returns:\n        mesh (Mesh): The mesh to use for sharding.\n        axis_names (tuple[str]): The axis names to use for sharding. The first axis\n            name is used for the data sharding, and the second axis name is used for\n            the model sharding.\n    \"\"\"\n    num_total_devices = len(jax.devices())\n    num_data_shards = jax.process_count()\n    num_model_shards = num_total_devices // num_data_shards\n\n    assert (\n        num_total_devices % num_data_shards == 0\n    ), \"Number of devices must be divisible by number of data shards\"\n\n    devices: np.ndarray[Any, Any] = np.array(jax.devices()).reshape(\n        (num_data_shards, num_model_shards)\n    )\n    axis_names = (\"data\", \"model\")\n    mesh = Mesh(devices, axis_names)\n\n    return mesh, axis_names\n</code></pre>"},{"location":"api/sharding/presets/#ddp-sharding","title":"DDP Sharding","text":"<p>The DDP sharding preset is a simple configuration that replicates the model across multiple devices and each replica recieves unique data. This configuration is ideal for training smaller models when you have multiple devices available.</p> DDP Sharding<pre><code>from loadax import DataloaderBuilder, InMemoryDataset, Batcher\nfrom loadax.sharding_utilities import ddp_sharding\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\n\nmesh, axis_names = ddp_sharding()\n\ndataloader = DataloaderBuilder(batcher)\n    .batch_size(2)\n    .shard(mesh, axis_names[0])\n    .build(dataset)\n</code></pre> <p>Default sharding configuration for DDP.</p> <p>DDP is a distributed training technique where the model's parameters are fully replicated across all devices, and the data is divided into shards, each processed by a separate node. This setup allows each device to handle unique subsets of the data while maintaining synchronized copies of the entire model. DDP is effective for scenarios where model size fits within the memory constraints of individual devices but benefits from data parallelism to accelerate training.</p> <p>Loadax's default DDP sharding configuration ensures that data is efficiently partitioned across nodes, and each device within a node holds a complete copy of the model's parameters. This configuration leverages JAX's parallelization capabilities to synchronize parameter updates across all replicas, ensuring consistency and facilitating scalable training.</p> <p>In the case of single-node training, this configuration is equivalent to DP training and does works perfectly fine for single-node training.</p> <p>This configuration is useful for DDP training, but you can also create your own sharding configurations. See the documentation for more information.</p> Source code in <code>src/loadax/sharding_utilities.py</code> <pre><code>def ddp_sharding() -&gt; ShardingPreset:\n    \"\"\"Default sharding configuration for DDP.\n\n    DDP is a distributed training technique where the model's parameters are fully\n    replicated across all devices, and the data is divided into shards, each processed\n    by a separate node. This setup allows each device to handle unique subsets of the\n    data while maintaining synchronized copies of the entire model. DDP is effective\n    for scenarios where model size fits within the memory constraints of individual\n    devices but benefits from data parallelism to accelerate training.\n\n    Loadax's default DDP sharding configuration ensures that data is efficiently\n    partitioned across nodes, and each device within a node holds a complete copy of the\n    model's parameters. This configuration leverages JAX's parallelization capabilities\n    to synchronize parameter updates across all replicas, ensuring consistency and\n    facilitating scalable training.\n\n    In the case of single-node training, this configuration is equivalent to DP training\n    and does works perfectly fine for single-node training.\n\n    This configuration is useful for DDP training, but you can also create your own\n    sharding configurations. See the documentation for more information.\n    \"\"\"\n    num_total_devices = len(jax.devices())\n    num_data_shards = jax.process_count()\n\n    assert (\n        num_total_devices % num_data_shards == 0\n    ), \"Number of devices must be divisible by number of data shards\"\n\n    devices: np.ndarray[Any, Any] = np.array(jax.devices()).reshape((num_data_shards,))\n    axis_names = (\"data\",)\n    mesh = Mesh(devices, axis_names)\n\n    return mesh, axis_names\n</code></pre>"},{"location":"api/sharding/sharding-strategy/","title":"Sharding Strategy","text":"<p>A sharding strategy is a simple interface that defines how to shard data across multiple devices and potentially multiple hosts. In the case of single-node training, this is not necessary, but in the case of distributed training, the ShardingStrategy informs loadax of how to partition the dataset across multiple hosts. Because loadax can use this information to pre-compute which data needs to be loaded on each host, it can then only load the data that is needed on each host, which can significantly improve data loading performance and reduce network traffic.</p> Creating a sharding strategy<pre><code>from loadax import InMemoryDataset, NoShardingStrategy\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nsharding_strategy = NoShardingStrategy()\n\nfor shard in sharding_strategy.get_shard_indices(dataset_size=5, shard_id=0, num_shards=1):\n    print(shard)\n\n#&gt; range(0, 5)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for sharding strategies.</p> <p>               Bases: <code>ShardingStrategy</code></p> <p>A sharding strategy that does not shard the data.</p> <p>This is the default sharding strategy used on single node training. This also is useful if you are doing ensemble training where each node should get the exact same batch of data.</p> <p>               Bases: <code>ShardingStrategy</code></p> <p>Define a distributed sharding strategy for the dataloader.</p> <p>This sharding strategy is useful for distributed training where each node should only load a partition of the dataset. This is done by dividing the dataset into equally sized shards and assigning each worker node a subset of the shards.</p> <p>The most common way to use this sharding strategy is to have each jax process load a <code>local_batch</code> which is a partition of the batch at any given step, because in most distributed training setups, the data is not replicated across the entire cluster. This means that we can pre-compute which process needs which data and then we can just load the data for that process. Upon loading the <code>local_batch</code>, we can then use the <code>distribute_global_batch</code> method to distribute the <code>local_batch</code> across the entire cluster. This name is atually a bit misleading, because the <code>local_batch</code>is not actually distributed across the entire cluster, but rather is just stitched into a <code>global_batch</code> that is distributed across the entire cluster. This enables you to write your jax code in a way that is agnostic to the distribution of the data.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Mesh</code> <p>The mesh to use for sharding.</p> required <code>data_shard_axis</code> <code>str | None</code> <p>The axis name for the data sharding. This is used to determine how to compute the global batch from the node-local batches. If None, then the data sharding axis will be required to be supplied when using <code>distribute_global_batch</code>.</p> <code>None</code> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def __init__(self, mesh: Mesh, data_shard_axis: str | None = None):\n    \"\"\"Create a new distributed sharding strategy.\n\n    Args:\n        mesh (Mesh): The mesh to use for sharding.\n        data_shard_axis (str | None): The axis name for the data sharding.\n            This is used to determine how to compute the global batch from\n            the node-local batches. If None, then the data sharding axis\n            will be required to be supplied when using `distribute_global_batch`.\n    \"\"\"\n    self.mesh = mesh\n    self.data_axis = data_shard_axis\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.ShardingStrategy.get_shard_indices","title":"get_shard_indices  <code>abstractmethod</code>","text":"<pre><code>get_shard_indices(dataset_size: int, shard_id: int, num_shards: int) -&gt; range\n</code></pre> <p>Get the data indices for a specific shard.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_size</code> <code>int</code> <p>The size of the dataset.</p> required <code>shard_id</code> <code>int</code> <p>The ID of the shard.</p> required <code>num_shards</code> <code>int</code> <p>The total number of shards.</p> required <p>Returns:</p> Name Type Description <code>range</code> <code>range</code> <p>The data indices for the shard.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>@abstractmethod\ndef get_shard_indices(\n    self, dataset_size: int, shard_id: int, num_shards: int\n) -&gt; range:\n    \"\"\"Get the data indices for a specific shard.\n\n    Args:\n        dataset_size: The size of the dataset.\n        shard_id: The ID of the shard.\n        num_shards: The total number of shards.\n\n    Returns:\n        range: The data indices for the shard.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.ShardingStrategy.needs_sharding","title":"needs_sharding  <code>abstractmethod</code>","text":"<pre><code>needs_sharding() -&gt; bool\n</code></pre> <p>Check if the dataloader needs to be sharded.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>@abstractmethod\ndef needs_sharding(self) -&gt; bool:\n    \"\"\"Check if the dataloader needs to be sharded.\"\"\"\n    pass\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.ShardingStrategy.distribute_global_batch","title":"distribute_global_batch  <code>abstractmethod</code>","text":"<pre><code>distribute_global_batch(local_batch: ndarray, *, data_axis: str | None = None) -&gt; ndarray\n</code></pre> <p>Distribute a local batch across the entire cluster.</p> <p>Take a local batch and stitch it together into a global batch with all processes contributing their local batch to the global batch. This is useful for distributed training where the data is not replicated across the entire cluster.</p> <p>Parameters:</p> Name Type Description Default <code>local_batch</code> <code>ndarray</code> <p>The local batch to distribute.</p> required <code>data_axis</code> <code>str | None</code> <p>The axis name for the data sharding. If None, then the data axis must have been provided to the sharding_strategy. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The global batch.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>@abstractmethod\ndef distribute_global_batch(\n    self, local_batch: jnp.ndarray, *, data_axis: str | None = None\n) -&gt; jnp.ndarray:\n    \"\"\"Distribute a local batch across the entire cluster.\n\n    Take a local batch and stitch it together into a global batch with all processes\n    contributing their local batch to the global batch. This is useful for\n    distributed training where the data is not replicated across the entire cluster.\n\n    Args:\n        local_batch: The local batch to distribute.\n        data_axis: The axis name for the data sharding.\n            If None, then the data axis must have been provided to the\n            sharding_strategy. Defaults to None.\n\n    Returns:\n        np.ndarray: The global batch.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.ShardingStrategy.named_sharding","title":"named_sharding","text":"<pre><code>named_sharding(*names: str | None) -&gt; NamedSharding\n</code></pre> <p>Get a NamedSharding object for the given axis names.</p> <p>This is just a useful convenience method for creating NamedSharding objects, just provide the axis names as they exist in the mesh to create the NamedSharding object.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str | None</code> <p>The axis names to use for the NamedSharding object.</p> <code>()</code> <p>Returns:</p> Type Description <code>NamedSharding</code> <p>jax.NamedSharding: The NamedSharding object for the given axis names.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def named_sharding(self, *names: str | None) -&gt; jax.NamedSharding:\n    \"\"\"Get a NamedSharding object for the given axis names.\n\n    This is just a useful convenience method for creating NamedSharding objects,\n    just provide the axis names as they exist in the mesh to create the\n    NamedSharding object.\n\n    Args:\n        *names (str | None): The axis names to use for the NamedSharding object.\n\n    Returns:\n        jax.NamedSharding: The NamedSharding object for the given axis names.\n    \"\"\"\n    return jax.NamedSharding(self.mesh, PartitionSpec(*names))  # type: ignore # jax PartitionSpec has missing types in current version\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.NoShardingStrategy.get_shard_indices","title":"get_shard_indices","text":"<pre><code>get_shard_indices(dataset_size: int, shard_id: int, num_shards: int) -&gt; range\n</code></pre> <p>Get the data indices for a specific shard.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def get_shard_indices(\n    self, dataset_size: int, shard_id: int, num_shards: int\n) -&gt; range:\n    \"\"\"Get the data indices for a specific shard.\"\"\"\n    return range(dataset_size)\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.NoShardingStrategy.needs_sharding","title":"needs_sharding","text":"<pre><code>needs_sharding() -&gt; bool\n</code></pre> <p>Check if the dataloader needs to be sharded.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def needs_sharding(self) -&gt; bool:\n    \"\"\"Check if the dataloader needs to be sharded.\"\"\"\n    return False\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.NoShardingStrategy.distribute_global_batch","title":"distribute_global_batch","text":"<pre><code>distribute_global_batch(local_batch: ndarray, *, data_axis: str | None = None) -&gt; ndarray\n</code></pre> <p>Distribute a local batch across the entire cluster.</p> <p>Take a local batch and stitch it together into a global batch with all processes contributing their local batch to the global batch. This is useful for distributed training where the data is not replicated across the entire cluster.</p> <p>Parameters:</p> Name Type Description Default <code>local_batch</code> <code>ndarray</code> <p>The local batch to distribute.</p> required <code>data_axis</code> <code>str | None</code> <p>The axis name for the data sharding. If None, then the data axis must have been provided to the sharding_strategy. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The global batch.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def distribute_global_batch(\n    self, local_batch: jnp.ndarray, *, data_axis: str | None = None\n) -&gt; jnp.ndarray:\n    \"\"\"Distribute a local batch across the entire cluster.\n\n    Take a local batch and stitch it together into a global batch with all processes\n    contributing their local batch to the global batch. This is useful for\n    distributed training where the data is not replicated across the entire cluster.\n\n    Args:\n        local_batch: The local batch to distribute.\n        data_axis: The axis name for the data sharding.\n            If None, then the data axis must have been provided to the\n            sharding_strategy. Defaults to None.\n\n    Returns:\n        np.ndarray: The global batch.\n    \"\"\"\n    return local_batch\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.DistributedShardingStrategy.get_shard_indices","title":"get_shard_indices","text":"<pre><code>get_shard_indices(dataset_size: int, shard_id: int, num_shards: int) -&gt; range\n</code></pre> <p>Get the data indices for a specific shard.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_size</code> <code>int</code> <p>The size of the dataset.</p> required <code>shard_id</code> <code>int</code> <p>The ID of the shard.</p> required <code>num_shards</code> <code>int</code> <p>The total number of shards.</p> required <p>Returns:</p> Name Type Description <code>range</code> <code>range</code> <p>The data indices for the shard.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def get_shard_indices(\n    self, dataset_size: int, shard_id: int, num_shards: int\n) -&gt; range:\n    \"\"\"Get the data indices for a specific shard.\n\n    Args:\n        dataset_size: The size of the dataset.\n        shard_id: The ID of the shard.\n        num_shards: The total number of shards.\n\n    Returns:\n        range: The data indices for the shard.\n    \"\"\"\n    if num_shards &lt;= 1:\n        return range(dataset_size)\n\n    base_shard_size = dataset_size // num_shards\n    remainder = dataset_size % num_shards\n\n    # distribute the remainder evenly across the first 'remainder' shards\n    if shard_id &lt; remainder:\n        shard_size = base_shard_size + 1\n        start_index = shard_id * shard_size\n    else:\n        shard_size = base_shard_size\n        start_index = (\n            remainder * (base_shard_size + 1) + (shard_id - remainder) * shard_size\n        )\n\n    end_index = start_index + shard_size\n    return range(start_index, end_index)\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.DistributedShardingStrategy.needs_sharding","title":"needs_sharding","text":"<pre><code>needs_sharding() -&gt; bool\n</code></pre> <p>Check if the dataloader needs to be sharded.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def needs_sharding(self) -&gt; bool:\n    \"\"\"Check if the dataloader needs to be sharded.\"\"\"\n    return True\n</code></pre>"},{"location":"api/sharding/sharding-strategy/#loadax.dataloader.sharding.DistributedShardingStrategy.distribute_global_batch","title":"distribute_global_batch","text":"<pre><code>distribute_global_batch(local_batch: ndarray, *, data_axis: str | None = None) -&gt; ndarray\n</code></pre> <p>Distribute a local batch across the entire cluster.</p> <p>Take a local batch and stitch it together into a global batch with all processes contributing their local batch to the global batch. This is useful for distributed training where the data is not replicated across the entire cluster.</p> <p>Parameters:</p> Name Type Description Default <code>local_batch</code> <code>ndarray</code> <p>The local batch to distribute.</p> required <code>data_axis</code> <code>str | None</code> <p>The axis name for the data sharding. If None, then the data axis must have been provided to the sharding_strategy. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The global batch.</p> Source code in <code>src/loadax/dataloader/sharding.py</code> <pre><code>def distribute_global_batch(\n    self, local_batch: jnp.ndarray, *, data_axis: str | None = None\n) -&gt; jnp.ndarray:\n    \"\"\"Distribute a local batch across the entire cluster.\n\n    Take a local batch and stitch it together into a global batch with all processes\n    contributing their local batch to the global batch. This is useful for\n    distributed training where the data is not replicated across the entire cluster.\n\n    Args:\n        local_batch: The local batch to distribute.\n        data_axis: The axis name for the data sharding.\n            If None, then the data axis must have been provided to the\n            sharding_strategy. Defaults to None.\n\n    Returns:\n        np.ndarray: The global batch.\n    \"\"\"\n    # local_batch: the batch of data on the local process\n    # data_axis: the axis name for the data sharding\n\n    data_axis = data_axis or self.data_axis\n    if not data_axis:\n        raise ValueError(\n            \"data_axis must be provided when using distribute_global_batch\"\n        )\n\n    if jax.process_count() == 1:\n        return jnp.asarray(local_batch)\n\n    data_sharding = self.named_sharding(data_axis)\n    global_batch_size = local_batch.shape[0] * jax.process_count()\n    global_batch_shape = (global_batch_size,) + local_batch.shape[1:]\n\n    def data_callback(index: Any) -&gt; jnp.ndarray:\n        # index: the global index for the sharded array\n        # We need to return the local data corresponding to this index\n        # Since each process only has its local data, we check if the index\n        # corresponds to our process\n\n        # Calculate the start and end indices for this process\n        per_process_batch_size = local_batch.shape[0]\n        start = jax.process_index() * per_process_batch_size\n        end = start + per_process_batch_size\n\n        # Check if the requested index overlaps with our local data\n        index_start = index[0].start or 0\n        index_stop = index[0].stop or global_batch_size\n\n        # If the requested index is within our local data range, return\n        # the corresponding data\n        if start &lt;= index_start &lt; end:\n            local_index_start = index_start - start\n            local_index_stop = min(index_stop - start, per_process_batch_size)\n            return local_batch[local_index_start:local_index_stop]\n        else:\n            # Return an empty array if the index does not correspond to our data\n            return jnp.zeros((0,) + local_batch.shape[1:], dtype=local_batch.dtype)\n\n    global_batch = jax.make_array_from_callback(\n        global_batch_shape, data_sharding, data_callback\n    )\n    return global_batch\n</code></pre>"},{"location":"api/strategy/batch/","title":"Batch Strategy","text":"<p>A batch strategy is a simple interface that defines how to collate batches from a dataset. The batch strategy is responsible for determining how to batch data from a stream of elements. Currently, loadax only supports the <code>FixedBatchStrategy</code>, which batches data into fixed size batches. However, in the future, loadax will support other batch strategies such as variable batch sizes, and even variable batch sizes that are determined by the data itself.</p> Creating a batch strategy<pre><code>from loadax import Batcher, InMemoryDataset, FixedBatchStrategy\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\nbatch_strategy = FixedBatchStrategy(batch_size=2)\n\nfor batch in batch_strategy.batch(dataset):\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>               Bases: <code>Protocol</code>, <code>Generic[DatasetItem]</code></p> <p>The method for determining how much data goes into a batch.</p> <p>This protocol defines the interface for determining how much data goes into a batch as it is loaded from the dataset.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The size of the batch.</p>"},{"location":"api/strategy/batch/#loadax.strategy.BatchStrategy.add","title":"add","text":"<pre><code>add(item: DatasetItem) -&gt; None\n</code></pre> <p>Add an item to the batch.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>DatasetItem</code> <p>The item to add to the batch.</p> required Source code in <code>src/loadax/strategy.py</code> <pre><code>def add(self, item: DatasetItem) -&gt; None:\n    \"\"\"Add an item to the batch.\n\n    Args:\n        item: The item to add to the batch.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/batch/#loadax.strategy.BatchStrategy.batch","title":"batch","text":"<pre><code>batch(*, force: bool) -&gt; list[DatasetItem] | None\n</code></pre> <p>Get the batch.</p> <p>This method method returns the batch if it is full, or None if the batch is not full.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>Whether to force the batch to be returned even if it is not full.</p> required <p>Returns:</p> Type Description <code>list[DatasetItem] | None</code> <p>The batch, or None if the batch is not full.</p> Source code in <code>src/loadax/strategy.py</code> <pre><code>def batch(self, *, force: bool) -&gt; list[DatasetItem] | None:\n    \"\"\"Get the batch.\n\n    This method method returns the batch if it is full, or None if the batch is not\n    full.\n\n    Args:\n        force: Whether to force the batch to be returned even if it is not full.\n\n    Returns:\n        The batch, or None if the batch is not full.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/batch/#loadax.strategy.BatchStrategy.clone","title":"clone","text":"<pre><code>clone() -&gt; BatchStrategy[DatasetItem]\n</code></pre> <p>Clone the batch strategy.</p> <p>This is likely not necessary but exists in advanced use cases such as sharing a batch strategy between multiple processes.</p> <p>Returns:</p> Type Description <code>BatchStrategy[DatasetItem]</code> <p>A clone of the batch strategy.</p> Source code in <code>src/loadax/strategy.py</code> <pre><code>def clone(self) -&gt; \"BatchStrategy[DatasetItem]\":\n    \"\"\"Clone the batch strategy.\n\n    This is likely not necessary but exists in advanced use cases such as sharing\n    a batch strategy between multiple processes.\n\n    Returns:\n        A clone of the batch strategy.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/fixed/","title":"Fixed Batch Strategy","text":"<p>The FixedBatchStrategy is a simple batch strategy that batches data into fixed size batches. This ensures all batches are the same size, except for the last batch which may be smaller.</p> Creating a fixed batch strategy<pre><code>from loadax import Batcher, InMemoryDataset, FixedBatchStrategy\n\ndataset = InMemoryDataset([1, 2, 3, 4, 5])\nbatcher = Batcher(lambda x: x)\nbatch_strategy = FixedBatchStrategy(batch_size=2)\n\nfor batch in batch_strategy.batch(dataset):\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>               Bases: <code>BatchStrategy[DatasetItem]</code></p> <p>A batch strategy that batches items into a fixed size batch.</p> <p>This batch strategy batches items into a fixed size batch. It will always return the same batch size, and will return None if the batch is not full. This is almost always what you want.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <p>The size of the batch.</p> <code>items</code> <code>deque[DatasetItem]</code> <p>The items in the batch.</p> <p>This batch strategy batches items into a fixed size batch. It will always return the same batch size, and will return None if the batch is not full. This is almost always what you want.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch.</p> required Source code in <code>src/loadax/strategy.py</code> <pre><code>def __init__(self, batch_size: int):\n    \"\"\"A batch strategy that batches items into a fixed size batch.\n\n    This batch strategy batches items into a fixed size batch. It will always return\n    the same batch size, and will return None if the batch is not full. This is\n    almost always what you want.\n\n    Args:\n        batch_size: The size of the batch.\n    \"\"\"\n    self.batch_size = batch_size\n    self.items: deque[DatasetItem] = deque()\n</code></pre>"},{"location":"api/strategy/fixed/#loadax.strategy.FixedBatchStrategy.add","title":"add","text":"<pre><code>add(item: DatasetItem) -&gt; None\n</code></pre> <p>Add an item to the batch.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>DatasetItem</code> <p>The item to add to the batch.</p> required Source code in <code>src/loadax/strategy.py</code> <pre><code>def add(self, item: DatasetItem) -&gt; None:\n    \"\"\"Add an item to the batch.\n\n    Args:\n        item: The item to add to the batch.\n    \"\"\"\n    self.items.append(item)\n</code></pre>"},{"location":"api/strategy/fixed/#loadax.strategy.FixedBatchStrategy.batch","title":"batch","text":"<pre><code>batch(*, force: bool) -&gt; list[DatasetItem] | None\n</code></pre> <p>Get the batch.</p> <p>This method method returns the batch if it is full, or None if the batch is not full.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>Whether to force the batch to be returned even if it is not full.</p> required <p>Returns:</p> Type Description <code>list[DatasetItem] | None</code> <p>The batch, or None if the batch is not full.</p> Source code in <code>src/loadax/strategy.py</code> <pre><code>def batch(self, *, force: bool) -&gt; list[DatasetItem] | None:\n    \"\"\"Get the batch.\n\n    This method method returns the batch if it is full, or None if the batch is not\n    full.\n\n    Args:\n        force: Whether to force the batch to be returned even if it is not full.\n\n    Returns:\n        The batch, or None if the batch is not full.\n    \"\"\"\n    if len(self.items) &gt;= self.batch_size:\n        return [self.items.popleft() for _ in range(self.batch_size)]\n    elif force and self.items:\n        return list(self.items)\n    return None\n</code></pre>"},{"location":"api/strategy/fixed/#loadax.strategy.FixedBatchStrategy.clone","title":"clone","text":"<pre><code>clone() -&gt; FixedBatchStrategy[DatasetItem]\n</code></pre> <p>Clone the batch strategy.</p> <p>This is likely not necessary but exists in advanced use cases such as sharing a batch strategy between multiple processes.</p> <p>Returns:</p> Type Description <code>FixedBatchStrategy[DatasetItem]</code> <p>A clone of the batch strategy.</p> Source code in <code>src/loadax/strategy.py</code> <pre><code>def clone(self) -&gt; \"FixedBatchStrategy[DatasetItem]\":\n    \"\"\"Clone the batch strategy.\n\n    This is likely not necessary but exists in advanced use cases such as sharing\n    a batch strategy between multiple processes.\n\n    Returns:\n        A clone of the batch strategy.\n    \"\"\"\n    new_strategy: FixedBatchStrategy[DatasetItem] = FixedBatchStrategy(\n        self.batch_size\n    )\n    new_strategy.items = self.items.copy()\n    return new_strategy\n</code></pre>"}]}