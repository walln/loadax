{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Loadax","text":"<p>Loadax is a dataloading library designed for the JAX ecosystem. It provides utilities for feeding data into your training loop without having to worry about batching, shuffling, and other preprocessing steps. Loadax also handles background prefetching to improve performance, and distriubted data loading to train on multiple devices and even multiple hosts.</p> Loadax Example<pre><code>from loadax import Dataloader, SimpleDataset\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\ndataloader = Dataloader(dataset, batch_size=2)\n\nfor batch in loader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add loadax\n</code></pre>"},{"location":"multi-host/","title":"Multi-Host Training","text":"<p>Loadax provides a simple interface for defining your dataloading strategy for distributed training. This means that you can easily train your models on multiple hosts, and load data in parallel across multiple hosts. Loadax also provides a few common sharding configurations that can be used out of the box, but you can also create your own sharding configurations using JAX's <code>Mesh</code> and <code>NamedSharding</code> primitives.</p> <p>Loadax's <code>Dataloader</code> will automatically determine which elements to load on each host within the network ensuring that the data is evenly distributed, and each host only gets the data it needs. This requires no manual configuration, replication, or network topology changes.</p> Creating a distributed dataloader<pre><code>from loadax import Dataloader, SimpleDataset, ShardedDataset\nfrom loadax.sharding.presets import make_fsdp_mesh_config # or make your own\n\nconfig = make_fsdp_mesh_config(axis_names=(\"data\", \"model\"), batch_axis_name=\"data\")\nmesh = config.create_device_mesh()\n\n# You can use jax.process_index() to get the rank of the current host and jax.process_count() to get the total number of hosts.\ndataset = SimpleDataset([1, 2, 3, 4, 5]).split_dataset_by_node(world_size=2, rank=0)\ndataloader = Dataloader(dataset, batch_size=2)\n\nwith mesh:\n    for batch in dataloader:\n        print(batch)\n</code></pre> <p>See documentation for sharding presets to learn about common configurations such as FSDP and DDP. Or checkout the examples to learn more.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Loadax is a work in progress, and there are many features that are planned for the future. The current focus is on building a quality set of integrations with common dataset formats such as HuggingFace, Polars, and SQLite.</p> <p>Here is a list of planned features:</p> <ul> <li>Support for more dataset formats</li> <li>Variable batching</li> <li>Throughput monitoring</li> <li>Better integration with the JAX profiling tools</li> <li>Utilities for working with TPUs</li> </ul> <p>If you have any ideas for new features, please open an issue or submit a pull request.</p>"},{"location":"api/loader/","title":"Dataloader","text":"<p>The Dataloader is the main interface for loading data into your training loop. The Dataloader is responsible for defining how to efficiently load data from a dataset and allocate it to the appropriate devices, batches, and all of the other features that make up proper data loading.</p> <p>The Dataloader works by spawning background workers to prefetch data into a cache, and then filling batches from the cache as they become available. The use of background workers allows the dataloader to be highly efficient and not block the main thread, which is important for training loops. Loadax takes care of the parllelization details for you, so your dataloading is fast, reliable, and simple. The background cache will load out of order, as utilizes mutlithreading to load data in parallel, however the actual batches will be in order. This is because loadax ensures deterministic ordering of batches, and the background workers will load batches in the order that they are requested.</p> Creating a dataloader<pre><code>from loadax import Dataloader, SimpleDataset\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\ndataloader = Dataloader(\n    dataset=dataset,\n    batch_size=2,\n    num_workers=2,\n    prefetch_factor=2,\n)\nfor batch in dataloader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>               Bases: <code>Generic[Example]</code></p> <p>Dataloader that loads batches in the background or synchronously.</p> Example <pre><code>from loadax.experimental.dataset.simple import SimpleDataset\nfrom loadax.experimental.loader import Dataloader\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\ndataloader = Dataloader(\n    dataset=dataset,\n    batch_size=2,\n    num_workers=2,\n    prefetch_factor=2,\n    drop_last=False,\n)\nfor batch in dataloader:\n    print(batch)\n\n#&gt; [1, 2]\n#&gt; [3, 4]\n#&gt; [5]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to load data from.</p> required <code>batch_size</code> <code>int</code> <p>The size of each batch.</p> required <code>num_workers</code> <code>int</code> <p>The number of workers to use for parallel data loading. If 0, data will be loaded synchronously.</p> <code>0</code> <code>prefetch_factor</code> <code>int</code> <p>The prefetch factor to use for prefetching. If 0, no prefetching will occur.</p> <code>0</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>False</code> Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset[Example],\n    batch_size: int,\n    num_workers: int = 0,\n    prefetch_factor: int = 0,\n    *,\n    drop_last: bool = False,\n):\n    \"\"\"A dataloader that can load data in the background or synchronously.\n\n    Example:\n        ```python\n        from loadax.experimental.dataset.simple import SimpleDataset\n        from loadax.experimental.loader import Dataloader\n\n        dataset = SimpleDataset([1, 2, 3, 4, 5])\n        dataloader = Dataloader(\n            dataset=dataset,\n            batch_size=2,\n            num_workers=2,\n            prefetch_factor=2,\n            drop_last=False,\n        )\n        for batch in dataloader:\n            print(batch)\n\n        #&gt; [1, 2]\n        #&gt; [3, 4]\n        #&gt; [5]\n        ```\n\n    Args:\n        dataset (Dataset): The dataset to load data from.\n        batch_size (int): The size of each batch.\n        num_workers (int): The number of workers to use for parallel data loading.\n            If 0, data will be loaded synchronously.\n        prefetch_factor (int): The prefetch factor to use for prefetching.\n            If 0, no prefetching will occur.\n        drop_last (bool): Whether to drop the last incomplete batch.\n    \"\"\"\n    self.dataset = dataset\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.prefetch_factor = prefetch_factor\n    self.drop_last = drop_last\n</code></pre> <p>               Bases: <code>Generic[Example]</code></p> <p>Iterator for the dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>Dataloader</code> <p>The dataloader to iterate over.</p> required Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def __init__(self, dataloader: \"Dataloader[Example]\"):\n    \"\"\"Iterator for the dataloader.\n\n    Args:\n        dataloader (Dataloader): The dataloader to iterate over.\n    \"\"\"\n    self.dataloader = dataloader\n    self.current_index = 0\n    self.buffer = Queue(maxsize=max(1, self.dataloader.prefetch_factor))\n    self.exception = None\n\n    if self.dataloader.num_workers &gt; 0:\n        self.executor = ThreadPoolExecutor(max_workers=self.dataloader.num_workers)\n        self.stop_event = threading.Event()\n        self.prefetch_thread = threading.Thread(target=self._prefetch_worker)\n        self.prefetch_thread.start()\n    else:\n        self.executor = None\n</code></pre>"},{"location":"api/loader/#loadax.dataloader.loader.DataloaderIterator.progress","title":"progress","text":"<pre><code>progress() -&gt; Progress\n</code></pre> <p>Get the progress of the dataloader.</p> Source code in <code>src/loadax/dataloader/loader.py</code> <pre><code>def progress(self) -&gt; Progress:\n    \"\"\"Get the progress of the dataloader.\"\"\"\n    total_items = len(self.dataloader.dataset)\n    processed_items = min(self.current_index, total_items)\n    return Progress(processed_items, total_items)\n</code></pre>"},{"location":"api/progress/","title":"Progress","text":"<p>Progress is a simple interface that allows you to track the progress of a dataloader. Because a dataloader is a stateful iterator, you can use the <code>progress()</code> method to get the current progress of the iterator. This means no more manual calculations of the progress, dealing with batch sizes, or forgetting to update your progress tracking.</p> Tracking progress<pre><code>from loadax import Dataloader, SimpleDataset\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\ndataloder = Dataloader(dataset, batch_size=2)\n\niterator = iter(dataloader)\n\nfor batch in iterator:\n    print(batch)\n    iterator.progress()\n\n#&gt; [1, 2]\n#&gt; Progress(items_processed=2, items_total=5)\n#&gt; [3, 4]\n#&gt; Progress(items_processed=4, items_total=5)\n#&gt; [5]\n#&gt; Progress(items_processed=5, items_total=5)\n</code></pre> <p>Progress metadata for a dataloader.</p> <p>This metadata indicates how far the dataloader has progressed. This is useful for debugging and monitoring the progress of a dataloader.</p> <p>Attributes:</p> Name Type Description <code>items_processed</code> <code>int</code> <p>The number of items processed so far.</p> <code>items_total</code> <code>int</code> <p>The total number of items in the dataset.</p>"},{"location":"api/dataset/dataset/","title":"Dataset","text":"<p>A dataset is a simple interface that defines how to load data from a source. All datasets must implement this interface, and it is the responsibility of the dataset to load the data from the underlying source. Because datasets support random access, they should also know their size.</p> Creating a dataset<pre><code>from loadax import Dataset\n\nclass MyDataset(Dataset[int]):\n    def __init__(self, items: list[int]):\n        self.items = items\n\n    def get(self, index: int) -&gt; int:\n        return self.items[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\ndataset = MyDataset([1, 2, 3, 4, 5])\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[Example]</code></p> <p>Dataset is the basic protocol that loadax needs to support.</p> <p>Any random-access dataset can be used with loadax as long as its size can be known and elements can be retrieved by index.</p>"},{"location":"api/dataset/dataset/#loadax.dataset.dataset.Dataset.map","title":"map","text":"<pre><code>map(transform: Callable[[Example], Transformed]) -&gt; Dataset[Transformed]\n</code></pre> <p>Apply a transformation to each element in the dataset.</p> Source code in <code>src/loadax/dataset/dataset.py</code> <pre><code>def map(\n    self, transform: Callable[[Example], Transformed]\n) -&gt; \"Dataset[Transformed]\":\n    \"\"\"Apply a transformation to each element in the dataset.\"\"\"\n    return MappedDataset(self, transform)\n</code></pre>"},{"location":"api/dataset/dataset/#loadax.dataset.dataset.Dataset.filter","title":"filter","text":"<pre><code>filter(predicate: Callable[[Example], bool]) -&gt; Dataset[Example]\n</code></pre> <p>Filter elements in the dataset based on a predicate.</p> Source code in <code>src/loadax/dataset/dataset.py</code> <pre><code>def filter(self, predicate: Callable[[Example], bool]) -&gt; \"Dataset[Example]\":\n    \"\"\"Filter elements in the dataset based on a predicate.\"\"\"\n    return FilteredDataset(self, predicate)\n</code></pre>"},{"location":"api/dataset/dataset/#loadax.dataset.dataset.Dataset.map_batch","title":"map_batch","text":"<pre><code>map_batch(transform: Callable[[list[Example]], Transformed], batch_size: int = 32) -&gt; Dataset[Transformed]\n</code></pre> <p>Apply a transformation to batches of elements in the dataset.</p> Source code in <code>src/loadax/dataset/dataset.py</code> <pre><code>def map_batch(\n    self, transform: Callable[[list[Example]], Transformed], batch_size: int = 32\n) -&gt; \"Dataset[Transformed]\":\n    \"\"\"Apply a transformation to batches of elements in the dataset.\"\"\"\n    return MappedBatchDataset(self, transform, batch_size)\n</code></pre>"},{"location":"api/dataset/simple/","title":"Simple","text":"<p>The Simple is a simple dataset that stores all underlying items in a list in memory. This is a simple dataset and is only useful for small datasets and debugging.</p> Creating an in-memory dataset<pre><code>from loadax import Simple\n\ndataset = Simple([1, 2, 3, 4, 5])\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n</code></pre> <p>               Bases: <code>Shardable[Example]</code>, <code>Shuffleable[Example]</code>, <code>Dataset[Example]</code>, <code>Generic[Example]</code></p> <p>A dataset that wraps a list of examples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Example]</code> <p>The list of data examples.</p> required <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Example]</code> <p>The list of data examples.</p> required Source code in <code>src/loadax/dataset/simple.py</code> <pre><code>def __init__(self, data: list[Example]):\n    \"\"\"Initialize a simple dataset in-memory from a list.\n\n    Args:\n        data (List[Example]): The list of data examples.\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"api/dataset/simple/#loadax.dataset.simple.SimpleDataset.split_dataset_by_node","title":"split_dataset_by_node","text":"<pre><code>split_dataset_by_node(world_size: int, rank: int) -&gt; Dataset[Example]\n</code></pre> <p>Split the dataset into shards.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The number of nodes.</p> required <code>rank</code> <code>int</code> <p>The rank of the current node.</p> required <p>Returns:</p> Type Description <code>Dataset[Example]</code> <p>Dataset[Example]: The shard of the dataset for the current node.</p> Source code in <code>src/loadax/dataset/simple.py</code> <pre><code>def split_dataset_by_node(self, world_size: int, rank: int) -&gt; Dataset[Example]:\n    \"\"\"Split the dataset into shards.\n\n    Args:\n        world_size (int): The number of nodes.\n        rank (int): The rank of the current node.\n\n    Returns:\n        Dataset[Example]: The shard of the dataset for the current node.\n    \"\"\"\n    start, end = compute_shard_boundaries(\n        num_shards=world_size,\n        shard_id=rank,\n        dataset_size=len(self),\n        drop_remainder=False,\n    )\n    return SimpleDataset(self.data[start:end])\n</code></pre>"},{"location":"api/dataset/simple/#loadax.dataset.simple.SimpleDataset.shuffle","title":"shuffle","text":"<pre><code>shuffle(seed: Array) -&gt; Dataset[Example]\n</code></pre> <p>Shuffle the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Array</code> <p>The seed to use for the shuffle. This is a jax PRNGKey as all randomization in loadax is implemented using jax.random.</p> required <p>Returns:</p> Type Description <code>Dataset[Example]</code> <p>The shuffled dataset.</p> Source code in <code>src/loadax/dataset/simple.py</code> <pre><code>def shuffle(self, seed: jax.Array) -&gt; \"Dataset[Example]\":\n    \"\"\"Shuffle the dataset.\n\n    Args:\n        seed: The seed to use for the shuffle. This is a jax\n            PRNGKey as all randomization in loadax is implemented using jax.random.\n\n    Returns:\n        The shuffled dataset.\n    \"\"\"\n    indices = jax.random.permutation(seed, len(self))\n    return SimpleDataset([self.data[i] for i in indices])\n</code></pre>"},{"location":"api/dataset/integration/huggingface/","title":"Huggingface Integration","text":"<p>Loadax provides integration with Huggingface Datasets. This integration allows you to load datasets from the Huggingface hub and use them with loadax.</p>"},{"location":"api/dataset/integration/huggingface/#loading-a-dataset","title":"Loading a Dataset","text":"<p>To load a dataset from the Huggingface hub, you can use the <code>from_hub</code> method of the <code>HuggingFaceDataset</code> class. This method takes in the path to the dataset on the Huggingface hub, the name of the dataset, and the split of the dataset. The dataset is lazily loaded from the Huggingface cache.</p> <pre><code>from loadax.dataset.huggingface import HuggingFaceDataset\n\ndataset = HuggingFaceDataset.from_hub(\"stanfordnlp/imdb\", split=\"train\")\n</code></pre> <p>Alternatively, you can construct a <code>HuggingFaceDataset</code> object directly from a Huggingface Dataset object. This is useful if you want to do some preprocessing and store the results in the Huggingface dataset cache.</p> <pre><code>from loadax.dataset.huggingface import HuggingFaceDataset\nimport datasets as hf_datasets\n\ntrain_data = hf_datasets.load_dataset(\"stanfordnlp/imdb\", split=\"train\")\ntrain_dataset = HuggingFaceDataset(train_data)\n</code></pre>"},{"location":"api/dataset/integration/huggingface/#sharding-a-dataset","title":"Sharding a Dataset","text":"<p>Huggingface datasets natively support sharding, no need to wrap them in a <code>ShardedDataset</code>. Instead you can use the <code>split_dataset_by_node</code> method to get a shard of the dataset for a given node. This method takes in the world size and the rank of the node and returns a shard of the dataset. The shards are contiguous and consistent for a given <code>world_size</code>.</p> <pre><code>from loadax.dataset.huggingface import HuggingFaceDataset\n\ndataset = HuggingFaceDataset.from_hub(\"stanfordnlp/imdb\", split=\"train\")\nshard = dataset.split_dataset_by_node(world_size=2, rank=0)\n</code></pre> <p>               Bases: <code>Shardable[Example]</code>, <code>Dataset[Example]</code></p> <p>A dataset that integrates with Hugging Face's <code>datasets</code> library.</p> <p>Any huggingface compatible dataset can be loaded with loadax to leverage the rich ecosystem of datasets, tooling, and efficient arrow-backed tables.</p> <p>If you are loading large datasets in a multi-host environment it is important to think about the order you load data for sharding. If you intend to shard your data such that each host is not fully replicated you will need to identify how to split the dataset.</p> <p>A huggingface dataset is sharded when loaded if using <code>HuggingFaceDataset.from_hub(..., num_shards=n, shard_id=i)</code>. Otherwise you will want to pre-shard it yourself.</p> <p>Examples:</p> <pre><code>from loadax.experimental.dataset.huggingface import HuggingFaceDataset\nimport datasets as hf_datasets\n\ntrain_data = hf_datasets.load_dataset(\"stanfordnlp/imdb\", split=\"train\")\ntrain_data.shard(num_shards=2, shard_id=0)\n\ntrain_dataset = HuggingFaceDataset(train_data)\n\ndata = train_dataset.get(0)\nprint(data)\n</code></pre> <p>Alternatively you can use ShardedDataset to wrap a HuggingFaceDataset. This will perform the same sharding algorithm as the datasets library however based on your usage it may not prevent the extra network overhead of loading all shards.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>HuggingFace Dataset</p> required Source code in <code>src/loadax/dataset/huggingface.py</code> <pre><code>def __init__(\n    self,\n    dataset: HFDataset,\n):\n    \"\"\"Initialize a huggingface dataset that has already been loaded.\n\n    Any huggingface compatible dataset can be loaded with loadax to leverage\n    the rich ecosystem of datasets, tooling, and efficient arrow-backed tables.\n\n    If you are loading large datasets in a multi-host environment it is important\n    to think about the order you load data for sharding. If you intend to shard\n    your data such that each host is not fully replicated you will need to\n    identify how to split the dataset.\n\n    A huggingface dataset is sharded when loaded if using\n    `HuggingFaceDataset.from_hub(..., num_shards=n, shard_id=i)`. Otherwise\n    you will want to pre-shard it yourself.\n\n    Examples:\n        ```python\n        from loadax.experimental.dataset.huggingface import HuggingFaceDataset\n        import datasets as hf_datasets\n\n        train_data = hf_datasets.load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n        train_data.shard(num_shards=2, shard_id=0)\n\n        train_dataset = HuggingFaceDataset(train_data)\n\n        data = train_dataset.get(0)\n        print(data)\n        ```\n\n    Alternatively you can use ShardedDataset to wrap a HuggingFaceDataset. This\n    will perform the same sharding algorithm as the datasets library however based\n    on your usage it may not prevent the extra network overhead of loading all\n    shards.\n\n    Args:\n        dataset: HuggingFace Dataset\n    \"\"\"\n    self._dataset = dataset\n</code></pre>"},{"location":"api/dataset/integration/huggingface/#loadax.dataset.huggingface.HuggingFaceDataset.dataset","title":"dataset  <code>property</code>","text":"<pre><code>dataset: Dataset\n</code></pre> <p>The underlying HuggingFace dataset.</p>"},{"location":"api/dataset/integration/huggingface/#loadax.dataset.huggingface.HuggingFaceDataset.from_hub","title":"from_hub  <code>staticmethod</code>","text":"<pre><code>from_hub(path: str, name: str | None = None, split: str | None = None) -&gt; HuggingFaceDataset[Example]\n</code></pre> <p>Load a HuggingFace dataset from the HuggingFace hub.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the dataset on the HuggingFace hub.</p> required <code>name</code> <code>str | None</code> <p>The name of the dataset on the HuggingFace hub.</p> <code>None</code> <code>split</code> <code>str | None</code> <p>The split of the dataset on the HuggingFace hub.</p> <code>None</code> <p>Returns:</p> Type Description <code>HuggingFaceDataset[Example]</code> <p>HuggingFaceDataset[Example]: The HuggingFace dataset.</p> <p>Examples:</p> <pre><code>from loadax.experimental.dataset.huggingface import HuggingFaceDataset\n\ndataset = HuggingFaceDataset.from_hub(\"stanfordnlp/imdb\")\n</code></pre> Source code in <code>src/loadax/dataset/huggingface.py</code> <pre><code>@staticmethod\ndef from_hub(\n    path: str,\n    name: str | None = None,\n    split: str | None = None,\n) -&gt; \"HuggingFaceDataset[Example]\":\n    \"\"\"Load a HuggingFace dataset from the HuggingFace hub.\n\n    Args:\n        path: The path to the dataset on the HuggingFace hub.\n        name: The name of the dataset on the HuggingFace hub.\n        split: The split of the dataset on the HuggingFace hub.\n\n    Returns:\n        HuggingFaceDataset[Example]: The HuggingFace dataset.\n\n    Examples:\n        ```python\n        from loadax.experimental.dataset.huggingface import HuggingFaceDataset\n\n        dataset = HuggingFaceDataset.from_hub(\"stanfordnlp/imdb\")\n        ```\n    \"\"\"\n    dataset = load_dataset(\n        path=path, name=name, split=split, trust_remote_code=True\n    )\n    dataset.set_format(type=\"numpy\")\n\n    assert isinstance(\n        dataset, HFDataset\n    ), f\"loaded dataset must be a Dataset, got {type(dataset)}\"\n\n    logger.info(f\"Loaded HF dataset with length: {len(dataset)}\")\n    return HuggingFaceDataset[Example](dataset)\n</code></pre>"},{"location":"api/dataset/integration/huggingface/#loadax.dataset.huggingface.HuggingFaceDataset.split_dataset_by_node","title":"split_dataset_by_node","text":"<pre><code>split_dataset_by_node(world_size: int, rank: int) -&gt; Dataset[Example]\n</code></pre> <p>Split the dataset into shards.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The number of nodes.</p> required <code>rank</code> <code>int</code> <p>The rank of the current node.</p> required <p>Returns:</p> Type Description <code>Dataset[Example]</code> <p>Dataset[Example]: The shard of the dataset for the current node.</p> Source code in <code>src/loadax/dataset/huggingface.py</code> <pre><code>def split_dataset_by_node(self, world_size: int, rank: int) -&gt; Dataset[Example]:\n    \"\"\"Split the dataset into shards.\n\n    Args:\n        world_size (int): The number of nodes.\n        rank (int): The rank of the current node.\n\n    Returns:\n        Dataset[Example]: The shard of the dataset for the current node.\n    \"\"\"\n    from datasets.distributed import (\n        split_dataset_by_node as hf_split_dataset_by_node,\n    )\n\n    dataset = hf_split_dataset_by_node(self._dataset, rank, world_size)\n    assert isinstance(dataset, HFDataset)\n    return HuggingFaceDataset[Example](dataset)\n</code></pre>"},{"location":"api/dataset/transformations/combined/","title":"CombinedDataset","text":"<p>A combined dataset is a simple dataset that combines multiple underlying datasets. The combined dataset will return the items from the first underlying dataset, then the items from the second underlying dataset, and so on.</p> Creating a combined dataset<pre><code>from loadax import CombinedDataset, SimpleDataset\n\ndataset1 = SimpleDataset([1, 2, 3, 4, 5])\ndataset2 = SimpleDataset([6, 7, 8, 9, 10])\ncombined_dataset = CombinedDataset(dataset1, dataset2)\n\nfor i in range(len(combined_dataset)):\n    print(combined_dataset.get(i))\n\n#&gt; 1\n#&gt; 2\n#&gt; 3\n#&gt; 4\n#&gt; 5\n#&gt; 6\n#&gt; 7\n#&gt; 8\n#&gt; 9\n#&gt; 10\n</code></pre> <p>               Bases: <code>Dataset[Example]</code>, <code>Generic[Example]</code></p> <p>A dataset that combines two datasets sequentially.</p> <p>This dataset type allows you to concatenate two datasets, creating a new dataset that contains all elements from the first dataset followed by all elements from the second dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset1</code> <code>Dataset[Example]</code> <p>The first dataset to be combined.</p> required <code>dataset2</code> <code>Dataset[Example]</code> <p>The second dataset to be combined.</p> required Source code in <code>src/loadax/dataset/combined_dataset.py</code> <pre><code>def __init__(self, dataset1: Dataset[Example], dataset2: Dataset[Example]):\n    \"\"\"Initialize the CombinedDataset.\n\n    Args:\n        dataset1: The first dataset to be combined.\n        dataset2: The second dataset to be combined.\n    \"\"\"\n    self.dataset1 = dataset1\n    self.dataset2 = dataset2\n</code></pre>"},{"location":"api/dataset/transformations/mapped/","title":"MappedDataset","text":"<p>A mapped dataset is a transformation that applies a functional transformation to the underlying dataset. The transformation is applied lazily, the underlying dataset is preserved. Because loadax performs dataloading in the background, this means it is acceptable to perform lightweight data augmentation or transformations on the dataset.</p> <p>If you have some complicated transformations you may still want to perform them ahead of time.</p> Creating a mapped dataset<pre><code>from loadax import MappedDataset, SimpleDataset\n\ndef transform(x):\n    return x * 2\n\ndataset = MappedDataset(SimpleDataset([1, 2, 3, 4, 5]), transform)\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 2\n#&gt; 4\n#&gt; 6\n#&gt; 8\n#&gt; 10\n</code></pre> <p>               Bases: <code>Dataset[Transformed]</code>, <code>Generic[Example, Transformed]</code></p> <p>A dataset that applies a transformation to each element in the dataset.</p> <p>The transformation is lazily applied, this means that the underlying data is not altered and instead is only applied when iterated over.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[Example]</code> <p>The underlying dataset to apply the transformation to.</p> required <code>transform</code> <code>Callable[[Example], Transformed]</code> <p>The transformation to apply to each element in the dataset.</p> required Source code in <code>src/loadax/dataset/dataset.py</code> <pre><code>def __init__(\n    self, dataset: Dataset[Example], transform: Callable[[Example], Transformed]\n):\n    \"\"\"Intializes the MappedDataset.\n\n    Args:\n        dataset: The underlying dataset to apply the transformation to.\n        transform: The transformation to apply to each element in the dataset.\n    \"\"\"\n    self.dataset = dataset\n    self.transform = transform\n</code></pre>"},{"location":"api/dataset/transformations/mapped/#mappedbatchdataset","title":"MappedBatchDataset","text":"<p>A mapped batch dataset is a transformation that applies a functional transformation to the underlying dataset. The transformation is applied lazily, the underlying dataset is preserved. Because loadax performs dataloading in the background, this means it is acceptable to perform lightweight data augmentation or transformations on the dataset. </p> <p>Similar to the <code>MappedDataset</code>, but the transformation is applied to batches of items instead of individual items. This is useful for performing batch-level transformations such as data augmentation or working with more expensive transformations that can be vectorized.</p> Creating a mapped batch dataset<pre><code>from loadax import MappedBatchDataset, SimpleDataset\n\ndef transform(batch):\n    return [item * 2 for item in batch]\n\ndataset = MappedBatchDataset(SimpleDataset([1, 2, 3, 4, 5]), transform)\n\nfor i in range(len(dataset)):\n    print(dataset.get(i))\n\n#&gt; 2\n#&gt; 4\n#&gt; 6\n#&gt; 8\n#&gt; 10\n</code></pre> <p>               Bases: <code>Dataset[Transformed]</code>, <code>Generic[Example, Transformed]</code></p> <p>Performs element transformations in batches.</p> <p>Just as with MappedDataset, the transformation is lazily applied, this means that the underlying data is not altered and instead is only applied when iterated over.</p> <p>Batched-mapping is useful when you want to apply a transformation that is particularly expensive to apply to a large number of elements. For example, if you have a dataset that needs to be tokenized, you can apply the tokenization to each batch of elements in the dataset to avoid the overhead of tokenizing each element individually.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[Example]</code> <p>The underlying dataset to apply the transformation to.</p> required <code>transform</code> <code>Callable[[list[Example]], Transformed]</code> <p>The transformation to apply to each batch of elements in the dataset.</p> required <code>batch_size</code> <code>int</code> <p>The size of each batch.</p> <code>32</code> Source code in <code>src/loadax/dataset/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset[Example],\n    transform: Callable[[list[Example]], Transformed],\n    batch_size: int = 32,\n):\n    \"\"\"Intializes the MappedBatchDataset.\n\n    Args:\n        dataset: The underlying dataset to apply the transformation to.\n        transform: The transformation to apply to each batch of elements in the\n            dataset.\n        batch_size: The size of each batch.\n    \"\"\"\n    self.dataset = dataset\n    self.transform = transform\n    self.batch_size = batch_size\n</code></pre>"},{"location":"api/dataset/transformations/partial/","title":"PartialDataset","text":"<p>A partial dataset is a simple dataset that returns a subset of the underlying dataset. This is useful for testing and debugging.</p> Creating a partial dataset<pre><code>from loadax import PartialDataset, SimpleDataset\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\npartial_dataset = PartialDataset(dataset, 2, 4)\n\nfor i in range(len(partial_dataset)):\n    print(partial_dataset.get(i))\n\n#&gt; 2\n#&gt; 3\n#&gt; 4\n</code></pre> <p>               Bases: <code>Dataset[Example]</code>, <code>Generic[Example]</code></p> <p>A dataset that represents a range of another dataset.</p> <p>This dataset type allows you to create a new dataset that contains a subset of elements from an existing dataset, specified by a start and end index.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[Example]</code> <p>The original dataset to create a partial view of.</p> required <code>start</code> <code>int</code> <p>The starting index of the range (inclusive).</p> required <code>end</code> <code>int</code> <p>The ending index of the range (exclusive).</p> required Source code in <code>src/loadax/dataset/partial_dataset.py</code> <pre><code>def __init__(self, dataset: Dataset[Example], start: int, end: int):\n    \"\"\"Initialize the PartialDataset.\n\n    Args:\n        dataset: The original dataset to create a partial view of.\n        start: The starting index of the range (inclusive).\n        end: The ending index of the range (exclusive).\n    \"\"\"\n    self.dataset = dataset\n    self.start = start\n    self.end = end\n\n    if start &lt; 0 or end &gt; len(dataset) or start &gt;= end:\n        raise ValueError(\"Invalid start or end index\")\n</code></pre>"},{"location":"api/dataset/transformations/partial/#loadax.dataset.partial_dataset.PartialDataset.split_dataset","title":"split_dataset  <code>staticmethod</code>","text":"<pre><code>split_dataset(dataset: Dataset[Example], num_partitions: int) -&gt; list[PartialDataset[Example]]\n</code></pre> <p>Split a dataset into a number of partial datasets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[Example]</code> <p>The original dataset to split.</p> required <code>num_partitions</code> <code>int</code> <p>The number of partitions to create.</p> required <p>Returns:</p> Type Description <code>list[PartialDataset[Example]]</code> <p>A list of PartialDataset objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_partitions is less than 1 or greater than the dataset size.</p> Source code in <code>src/loadax/dataset/partial_dataset.py</code> <pre><code>@staticmethod\ndef split_dataset(\n    dataset: Dataset[Example], num_partitions: int\n) -&gt; list[\"PartialDataset[Example]\"]:\n    \"\"\"Split a dataset into a number of partial datasets.\n\n    Args:\n        dataset: The original dataset to split.\n        num_partitions: The number of partitions to create.\n\n    Returns:\n        A list of PartialDataset objects.\n\n    Raises:\n        ValueError: If num_partitions is less than 1 or greater than the\n            dataset size.\n    \"\"\"\n    if num_partitions &lt; 1:\n        raise ValueError(\"Number of partitions must be at least 1\")\n    if num_partitions &gt; len(dataset):\n        raise ValueError(\"Number of partitions cannot exceed dataset size\")\n\n    partition_size = len(dataset) // num_partitions\n    remainder = len(dataset) % num_partitions\n\n    partials = []\n    start = 0\n    for i in range(num_partitions):\n        end = start + partition_size + (1 if i &lt; remainder else 0)\n        partials.append(PartialDataset(dataset, start, end))\n        start = end\n\n    return partials\n</code></pre>"},{"location":"api/dataset/transformations/sampled/","title":"SampledDataset","text":"<p>A sampled dataset is a simple dataset that returns a subset of the underlying dataset. The sampling is performed lazily and does not actually sample the underlying storage. If your underlying storage does not perform well with random access, you may want to consider performing the sampling in advance. However, for almost all use cases, this should not be necessary.</p> <p>The sampling procedure is deterministic and leverages JAX's random number generation.</p> Creating a sampled dataset<pre><code>from loadax import SampledDataset, SimpleDataset\nimport jax\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\nkey = jax.random.PRNGKey(0)\nsampled_dataset = SampledDataset(dataset, 3, key)\n</code></pre> <p>               Bases: <code>Dataset[Example]</code>, <code>Generic[Example]</code></p> <p>A dataset that represents a random sample of another dataset.</p> <p>This dataset type allows you to create a new dataset that contains a random subset of elements from an existing dataset, specified by a sample size and a random key.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[Example]</code> <p>The original dataset to create a sampled view of.</p> required <code>sample_size</code> <code>int</code> <p>The number of samples to include in the new dataset.</p> required <code>key</code> <code>Array</code> <p>The random key to use for sampling.</p> required Source code in <code>src/loadax/dataset/sampled_dataset.py</code> <pre><code>def __init__(self, dataset: Dataset[Example], sample_size: int, key: jax.Array):\n    \"\"\"Initialize the SampledDataset.\n\n    Args:\n        dataset: The original dataset to create a sampled view of.\n        sample_size: The number of samples to include in the new dataset.\n        key: The random key to use for sampling.\n    \"\"\"\n    self.dataset = dataset\n    self.sample_size = sample_size\n    self.key = key\n\n    if sample_size &lt; 0 or sample_size &gt; len(dataset):\n        raise ValueError(\"Invalid sample size\")\n\n    self.indices = jax.random.choice(\n        key, jnp.arange(len(dataset)), shape=(sample_size,), replace=False\n    )\n</code></pre>"},{"location":"api/dataset/transformations/sharded/","title":"ShardedDataset","text":"<p>A dataset that implements the <code>Shardable</code> protocol. This allows you to partition your dataset across multiple hosts.</p> Creating a sharded dataset<pre><code>from loadax import SimpleDataset, ShardedDataset\n\ndataset = SimpleDataset([1, 2, 3, 4, 5])\nsharded_dataset = ShardedDataset(dataset, num_shards=2, shard_id=0)\n</code></pre> <p>The shardable protocol requires you to implement the <code>split_dataset_by_node</code> method. This method should take in the <code>world_size</code> and the <code>rank</code> of the current host and return a shard of the dataset for that host.</p> <p>Loadax's provided datasets will implement this method for you.</p> <p>               Bases: <code>Dataset[Example]</code>, <code>Generic[Example]</code></p> <p>Divides the dataset into non-overlapping contiguous shards.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[E]</code> <p>The underlying dataset to shard.</p> required <code>num_shards</code> <code>int</code> <p>Total number of shards.</p> required <code>shard_id</code> <code>int</code> <p>The ID of the current shard (0-based).</p> required <code>drop_remainder</code> <code>bool</code> <p>Whether to drop the last incomplete shard. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>dataset</code> is not an instance of Dataset.</p> <code>ValueError</code> <p>If <code>num_shards</code> is not a positive integer.</p> <code>ValueError</code> <p>If <code>shard_id</code> is not in the range [0, num_shards).</p> <code>ValueError</code> <p>If <code>drop_remainder</code> is True and <code>dataset_size</code> &lt; <code>num_shards</code>.</p> Source code in <code>src/loadax/dataset/sharded_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset[Example],\n    num_shards: int,\n    shard_id: int,\n    *,\n    drop_remainder: bool = True,\n):\n    \"\"\"Initialize a ShardedDataset to shard the given dataset.\n\n    Args:\n        dataset (Dataset[E]): The underlying dataset to shard.\n        num_shards (int): Total number of shards.\n        shard_id (int): The ID of the current shard (0-based).\n        drop_remainder (bool, optional): Whether to drop the last incomplete shard.\n            Defaults to True.\n\n    Raises:\n        TypeError: If `dataset` is not an instance of Dataset.\n        ValueError: If `num_shards` is not a positive integer.\n        ValueError: If `shard_id` is not in the range [0, num_shards).\n        ValueError: If `drop_remainder` is True and `dataset_size` &lt; `num_shards`.\n    \"\"\"\n    if not isinstance(dataset, Shardable):\n        raise TypeError(\"dataset must implement the Shardable protocol.\")\n    if not isinstance(num_shards, int) or num_shards &lt;= 0:\n        raise ValueError(\"num_shards must be a positive integer.\")\n    if not isinstance(shard_id, int) or not (0 &lt;= shard_id &lt; num_shards):\n        raise ValueError(f\"shard_id must be an integer in [0, {num_shards}).\")\n\n    self.dataset = dataset\n    self.num_shards = num_shards\n    self.shard_id = shard_id\n    self.drop_remainder = drop_remainder\n    self.dataset_size = len(self.dataset)\n\n    if self.drop_remainder and self.dataset_size &lt; self.num_shards:\n        raise ValueError(\n            f\"dataset_size ({self.dataset_size}) must be &gt;= num_shards \"\n            f\"({self.num_shards}) when drop_remainder is True.\"\n        )\n\n    self.start, self.end = compute_shard_boundaries(\n        num_shards=self.num_shards,\n        shard_id=self.shard_id,\n        dataset_size=self.dataset_size,\n        drop_remainder=self.drop_remainder,\n    )\n\n    self._length = max(0, self.end - self.start)\n</code></pre> <p>               Bases: <code>Protocol</code>, <code>Generic[Example]</code></p> <p>A shardable dataset must implement the Shardable protocol.</p> <p>Each dataset has to implement sharding by itself because the underlying storage may have unique constraints to consider when creating the sharding boundaries.</p>"},{"location":"api/dataset/transformations/sharded/#loadax.dataset.sharded_dataset.ShardedDataset.shard_boundaries","title":"shard_boundaries","text":"<pre><code>shard_boundaries() -&gt; tuple[int, int]\n</code></pre> <p>Return the start and end boundaries of the shard.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple[int, int]: The (start, end) indices of the shard.</p> Source code in <code>src/loadax/dataset/sharded_dataset.py</code> <pre><code>def shard_boundaries(self) -&gt; tuple[int, int]:\n    \"\"\"Return the start and end boundaries of the shard.\n\n    Returns:\n        Tuple[int, int]: The (start, end) indices of the shard.\n    \"\"\"\n    return self.start, self.end\n</code></pre>"},{"location":"api/dataset/transformations/sharded/#loadax.dataset.sharded_dataset.Shardable.split_dataset_by_node","title":"split_dataset_by_node  <code>abstractmethod</code>","text":"<pre><code>split_dataset_by_node(world_size: int, rank: int) -&gt; Dataset[Example]\n</code></pre> <p>Split the dataset into shards.</p> <p>If possible the shards should be of equal size and non-overlapping and continguous.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The number of nodes.</p> required <code>rank</code> <code>int</code> <p>The rank of the current node.</p> required <p>Returns:</p> Type Description <code>Dataset[Example]</code> <p>Dataset[Example]: The shard of the dataset for the current node.</p> Source code in <code>src/loadax/dataset/sharded_dataset.py</code> <pre><code>@abstractmethod\ndef split_dataset_by_node(self, world_size: int, rank: int) -&gt; Dataset[Example]:\n    \"\"\"Split the dataset into shards.\n\n    If possible the shards should be of equal size and non-overlapping\n    and continguous.\n\n    Args:\n        world_size (int): The number of nodes.\n        rank (int): The rank of the current node.\n\n    Returns:\n        Dataset[Example]: The shard of the dataset for the current node.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sharding/mesh/","title":"Mesh","text":"<p>Loadax has a specialized mesh definition called a <code>HybridMeshShape</code>. This enables you to define your mesh with two contexts, the global topology (inter-node) and the local topology (intra-node). This makes it easier to reason about the placement of data across the network.</p> <p>To use loadax's mesh abstractions, you need to define a <code>MeshConfig</code>. This config tells loadax how to split up the work across the mesh. A <code>MeshConfig</code> must specify a <code>mesh_shape</code>, which is a <code>HybridMeshShape</code> and some annotations about the mesh axes. </p> <p>Typically you should not need to define a <code>MeshConfig</code> yourself. Instead you can rely on Loadax's automatic mesh discovery to find a good mesh shape for your cluster for a given parallelization strategy. See the Presets section for more details.</p> <p>A mesh shape for hybrid (i.e., ICI and DCN) parallelism.</p> <p>For example, with mesh axes (data, model): - Pure fsdp on a v4-8:     <pre><code>HybridMeshShape(ici_mesh_shape=(1, 4), dcn_mesh_shape=(1, 1))\n</code></pre> - Two-way data parallelism over 2 H100 nodes, and fsdp within-node:     <pre><code>HybridMeshShape(ici_mesh_shape=(1, 8), dcn_mesh_shape=(2, 1))\n</code></pre></p> <p>Sharding Mesh Configuration.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.HybridMeshShape.ici_mesh_shape","title":"ici_mesh_shape  <code>instance-attribute</code>","text":"<pre><code>ici_mesh_shape: MeshShape\n</code></pre> <p>The mesh shape for the ICI (inner-core) parallelism. This represents the host-local sharding.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.HybridMeshShape.dcn_mesh_shape","title":"dcn_mesh_shape  <code>instance-attribute</code>","text":"<pre><code>dcn_mesh_shape: MeshShape\n</code></pre> <p>The mesh shape for the DCN (data-parallel) parallelism. This represents the inter-host sharding.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.mesh_shape","title":"mesh_shape  <code>instance-attribute</code>","text":"<pre><code>mesh_shape: MeshShape | HybridMeshShape\n</code></pre> <p>If specified as a MeshShape, must have the same length as mesh_axis_names. Implicitly, this treats the mesh shape as the ICI mesh shape; we default to a DCN mesh shape that partitions the first non-singleton axis across granules (e.g. TPU slices or GPU nodes). If all axes are singletons, this implies a single-granule environment and therefore an all-1's DCN mesh shape.</p> <p>As an example on 2 H100 nodes, for mesh axes (pipeline, data, model) and a MeshShape of (1, 2, 8), we break the \"data\" axis across DCN -- this produces a DCN mesh shape (1, 2, 1) and an ICI mesh shape (1, 1, 8), i.e. 2-way data-parallelism across DCN, and 8-way model parallelism within-node (e.g. NVLink). If instead the MeshShape is provided as (2, 1, 8), we break along the \"pipeline\" axis, producing a DCN mesh shape of (2, 1, 1) and ICI mesh shape (1, 1, 8) for 2-way pipeline-parallelism across DCN and 8-way model parallelism within-node.</p> <p>If specified as a HybridMeshShape, each member must have the same length as mesh_axis_names.</p> <p>Use <code>mesh_rules</code> to set different mesh shapes depending on the hardware platform.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.mesh_axis_names","title":"mesh_axis_names  <code>instance-attribute</code>","text":"<pre><code>mesh_axis_names: Sequence[str]\n</code></pre> <p>The mesh axis names. The names can be referenced in ParameterSpec.mesh_axes.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.batch_axis_names","title":"batch_axis_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_axis_names: str | Sequence[str] = 'data'\n</code></pre> <p>Subset of mesh axis names over which leaves of the input batch are sharded.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.mesh_rules","title":"mesh_rules  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mesh_rules: Sequence[tuple[str, MeshShape | None]] | None = None\n</code></pre> <p>An optional list of (regex, MeshShape) pairs to override the default mesh configuration.</p> <p>This is useful when we want to use different mesh shapes depending on the device types (e.g., 'tpu-v4-128' vs. 'gpu-p4de.24xlarge-32').</p> <p>Given a <code>mesh_selector</code> string (usually representing the device type and set by user's launch script), the first rule that with a regex that matches the selector will determine the mesh shape.</p> <p>If no rule matches, the default mesh configuration will be used.</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.hosts","title":"hosts  <code>property</code>","text":"<pre><code>hosts: int\n</code></pre> <p>Returns the number of hosts in the mesh.</p> <p>This is just a simple wrapper around jax.process_count().</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.host_id","title":"host_id  <code>property</code>","text":"<pre><code>host_id: int\n</code></pre> <p>Returns the ID of the current host.</p> <p>This is just a simple wrapper around jax.process_index().</p>"},{"location":"api/sharding/mesh/#loadax.sharding.mesh_shape.MeshConfig.create_device_mesh","title":"create_device_mesh","text":"<pre><code>create_device_mesh(devices: list[Device] | None = None) -&gt; Mesh\n</code></pre> <p>Creates a Mesh object for the given devices.</p> <p>Parameters:</p> Name Type Description Default <code>devices</code> <code>list[Device] | None</code> <p>A list of devices to create a Mesh object for. If None, the default devices will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Mesh</code> <p>jax.sharding.Mesh: A Mesh object for the given devices.</p> Source code in <code>src/loadax/sharding/mesh_shape.py</code> <pre><code>def create_device_mesh(\n    self, devices: list[jax.Device] | None = None\n) -&gt; jax.sharding.Mesh:\n    \"\"\"Creates a Mesh object for the given devices.\n\n    Args:\n        devices (list[jax.Device] | None, optional): A list of devices to create a\n            Mesh object for. If None, the default devices will be used. Defaults to\n            None.\n\n    Returns:\n        jax.sharding.Mesh: A Mesh object for the given devices.\n    \"\"\"\n    from loadax.sharding.mesh_utils import create_device_mesh\n\n    return jax.sharding.Mesh(\n        devices or create_device_mesh(mesh_shape=self.mesh_shape),\n        tuple(self.mesh_axis_names),\n    )\n</code></pre>"},{"location":"api/sharding/placement/","title":"Placement Utilities","text":"<p>Loadax provides a few utilities to help with sharding and placement of data and models.</p> <p>First we should cover why these utilities are necessary.</p> <p>In a distributed setting jax wants to know about the placement of regions of data called \"shards\". Shards are ranges within an array that fit on a single device. When training on multiple devices, and especially when using multiple nodes, you want to simplify the synchronization and allow jax to handle as much as possible. A great way to do this is to have each node load a unique subset of your dataset. This will make every batch unique on each device. In a single node setup, this is a no-op and you would not need to shard your dataset. If you are using multiple nodes you then have the challenge of synchronizing your training such that each node trains on a unique subset of the data and the nodes coordinate the backpropagation and gradient updates.</p> <p>This is where Loadax's <code>host_to_global_device_array</code> comes in. This function will take an array and communicate with all other nodes in the network to create a \"global\" array that is the same across all devices. This is different than sharding the array because the data never actually moves. This function annotates the array with the placement of the data so that jax can treat each node's batch as a single larger global batch with the local batches stitched together.</p>"},{"location":"api/sharding/placement/#host_to_global_device_array","title":"host_to_global_device_array","text":"<p>This function takes an array and annotates it with the placement of the data so that jax can treat each node's batch as a single larger global batch with the local batches stitched together.</p> <p>Converts the given host device arrays to global device arrays.</p> <p>Must be called within the context of a Mesh.</p> <p>We cannot use <code>multihost_utils.host_local_array_to_global_array</code> since the local mesh may not be contiguous. According to yashkatariya@google.com, \"using <code>jax.make_array_from_single_device_arrays</code> is the right solution.\"</p> <p>Parameters:</p> Name Type Description Default <code>host_arrays</code> <code>Nested[Array]</code> <p>a nested tree of device arrays in host memory. Usually these present the per-host portion of the global input batch.</p> required <code>partition</code> <code>DataPartitionType</code> <p>how the global array should be partitioned.</p> <code>FULL</code> <p>Returns:</p> Type Description <code>Nested[Array]</code> <p>A nested tree with the same structure as <code>host_arrays</code>, but global device</p> <code>Nested[Array]</code> <p>arrays at the leaves. Each global device array is partitioned</p> <code>Nested[Array]</code> <p>according to <code>partition</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>if the given <code>partition</code> type is not supported.</p> Source code in <code>src/loadax/sharding/placement.py</code> <pre><code>def host_to_global_device_array(\n    host_arrays: Nested[jax.Array],\n    *,\n    partition: DataPartitionType = DataPartitionType.FULL,\n) -&gt; Nested[jax.Array]:\n    \"\"\"Converts the given host device arrays to global device arrays.\n\n    Must be called within the context of a Mesh.\n\n    We cannot use `multihost_utils.host_local_array_to_global_array` since the local\n    mesh may not be contiguous. According to yashkatariya@google.com,\n    \"using `jax.make_array_from_single_device_arrays` is the right solution.\"\n\n    Args:\n        host_arrays: a nested tree of device arrays in host memory. Usually these\n            present the per-host portion of the global input batch.\n        partition: how the global array should be partitioned.\n\n    Returns:\n        A nested tree with the same structure as `host_arrays`, but global device\n        arrays at the leaves. Each global device array is partitioned\n        according to `partition`.\n\n    Raises:\n        NotImplementedError: if the given `partition` type is not supported.\n    \"\"\"\n    mesh = thread_resources.env.physical_mesh\n    partition_spec = data_partition_type_to_spec(partition)\n\n    local_devices = mesh.local_devices\n\n    def put_to_devices_fully_partitioned(x: jax.Array) -&gt; list[jax.Array]:\n        len_local_devices = len(local_devices)\n        if x.shape[0] % len_local_devices != 0:\n            raise ValueError(\n                f\"({x.shape}) cannot be sharded across {len_local_devices} devices.\"\n            )\n        # np.reshape is faster than np.split, jnp.reshape, and jnp.split.\n        xs = np.reshape(\n            x, (len_local_devices, x.shape[0] // len_local_devices, *x.shape[1:])\n        )\n        return [\n            jax.device_put(x_i, device)\n            for x_i, device in zip(xs, local_devices, strict=False)\n        ]\n\n    def put_to_devices_replicated(x: jax.Array) -&gt; list[jax.Array]:\n        # Replicate `x` to every local device.\n        return [jax.device_put(x, device) for device in local_devices]\n\n    if partition == DataPartitionType.FULL:\n        put_to_devices = put_to_devices_fully_partitioned\n    elif partition == DataPartitionType.REPLICATED:\n        put_to_devices = put_to_devices_replicated\n    else:\n        raise NotImplementedError(f\"Unsupported partition: {partition}\")\n\n    device_arrays = jax.tree_util.tree_map(put_to_devices, host_arrays)\n    partition_specs = complete_partition_spec_tree(\n        jax.tree_util.tree_structure(host_arrays),\n        partition_spec,\n    )\n\n    def make_gda(\n        x: jax.Array, device_buffers: list[jax.Array], partition_spec: PartitionSpec\n    ) -&gt; jax.Array:\n        if partition == DataPartitionType.FULL:\n            global_batch_size = x.shape[0] * jax.process_count()\n        elif partition == DataPartitionType.REPLICATED:\n            global_batch_size = x.shape[0]\n        else:\n            raise NotImplementedError(f\"Unsupported partition: {partition}\")\n        global_shape = (global_batch_size, *list(x.shape[1:]))\n        return jax.make_array_from_single_device_arrays(\n            shape=global_shape,\n            sharding=jax.sharding.NamedSharding(mesh, partition_spec),\n            arrays=device_buffers,\n        )\n\n    return jax.tree_util.tree_map(make_gda, host_arrays, device_arrays, partition_specs)  # type: ignore\n</code></pre>"},{"location":"api/sharding/placement/#global_to_host_array","title":"global_to_host_array","text":"<p>The inverse of <code>host_to_global_device_array</code>. This function takes a global array and splits it into the local arrays for each node.</p> <p>Extracts host addressable rows from each Array in <code>global_arrays</code>.</p> <p>Parameters:</p> Name Type Description Default <code>global_arrays</code> <code>Nested[Array]</code> <p>A Nested[jax.Array]. Each leaf Array must have shape [global_batch_size, ...] with identical global_batch_size across arrays. The arrays must be partitioned in the same way and can be partitioned only along the batch axis.</p> required <code>partition</code> <code>DataPartitionType</code> <p>How the global array should be partitioned.</p> <code>FULL</code> <p>Returns:</p> Type Description <code>Nested[Array]</code> <p>A Nested[jax.Array] with the same structure as <code>global_array</code>. Each leaf</p> <code>Nested[Array]</code> <p>Array will have shape [host_batch_size, ...] where <code>host_batch_size</code> will be</p> <code>Nested[Array]</code> <p>equal to <code>global_batch_size</code> if the global Arrays are replicated or</p> <code>Nested[Array]</code> <p><code>global_batch_size // process_count</code> if the global Arrays are partitioned</p> <code>Nested[Array]</code> <p>across hosts.</p> Source code in <code>src/loadax/sharding/placement.py</code> <pre><code>def global_to_host_array(\n    global_arrays: Nested[jax.Array],\n    *,\n    partition: DataPartitionType = DataPartitionType.FULL,\n) -&gt; Nested[jax.Array]:\n    \"\"\"Extracts host addressable rows from each Array in `global_arrays`.\n\n    Args:\n        global_arrays: A Nested[jax.Array].\n            Each leaf Array must have shape [global_batch_size, ...] with identical\n            global_batch_size across arrays.\n            The arrays must be partitioned in the same way and can be partitioned\n            only along the batch axis.\n        partition: How the global array should be partitioned.\n\n    Returns:\n        A Nested[jax.Array] with the same structure as `global_array`. Each leaf\n        Array will have shape [host_batch_size, ...] where `host_batch_size` will be\n        equal to `global_batch_size` if the global Arrays are replicated or\n        `global_batch_size // process_count` if the global Arrays are partitioned\n        across hosts.\n    \"\"\"\n\n    def sort_global_shards(global_shards: list[jax.Shard]) -&gt; list[jax.Shard]:\n        # We should sort jax.Array.global_shards by using this function to guarantee\n        # round-trip equality of host_to_global_device_array and global_to_host_array.\n        # Shards are sorted in-place.\n        global_shards.sort(key=lambda shard: shard.index)\n        return global_shards\n\n    global_array_items = flatten_items(global_arrays)\n    if not global_array_items:\n        return global_arrays  # no leaf jax.Array.\n    first_path, first_value = global_array_items[0]\n    sorted_first_value_shards = sort_global_shards(list(first_value.global_shards))\n    first_value_shard_is_local = [\n        shard.data is not None for shard in sorted_first_value_shards\n    ]\n    batch_size = first_value.shape[0]\n\n    def get_local_array(path: str, value: jax.Array) -&gt; jax.Array:\n        if value.shape[0] != batch_size:\n            raise ValueError(\n                f\"Value batch size mismatch: {batch_size} @ {first_path} vs. \"\n                f\"{value.shape[0]} @ {path} of {shapes(global_arrays)}\"\n            )\n        sorted_value_shards = sort_global_shards(list(value.global_shards))\n        value_shard_is_local = [shard.data is not None for shard in sorted_value_shards]\n        if value_shard_is_local != first_value_shard_is_local:\n            raise ValueError(\n                f\"Value shard mismatch: {first_value_shard_is_local} @ {first_path} \"\n                f\"vs. {value_shard_is_local} @ {path}\"\n            )\n        local_data = [\n            shard.data for shard in sorted_value_shards if shard.data is not None\n        ]\n        if not local_data:\n            raise ValueError(f\"No local shard found: {sorted_value_shards}.\")\n        if partition == DataPartitionType.FULL:\n            # return ndarray its faster than jnp.concatenate\n            return np.concatenate(local_data, axis=0)  # type: ignore\n        elif partition == DataPartitionType.REPLICATED:\n            return local_data[0]  # type: ignore\n        else:\n            raise NotImplementedError(f\"Unsupported partition: {partition}\")\n\n    # TODO: jtu types are bad\n    return jax.tree_util.tree_map(  # type: ignore\n        get_local_array, tree_paths(global_arrays), global_arrays\n    )\n</code></pre>"},{"location":"api/sharding/placement/#with_sharding_constraint","title":"with_sharding_constraint","text":"<p>This is syntactic sugar that ensures a <code>with_sharding_constraint</code> is applied when inside a Mesh context.</p> <p>Syntax sugar for <code>jax.lax.with_sharding_constraint</code>.</p> <p>Used from within the context of a Mesh, this will produce a no-op if the Mesh is empty or has only one device.</p> Source code in <code>src/loadax/sharding/placement.py</code> <pre><code>def with_sharding_constraint(x: jax.Array, shardings: Any) -&gt; jax.Array:\n    \"\"\"Syntax sugar for `jax.lax.with_sharding_constraint`.\n\n    Used from within the context of a Mesh, this will produce a no-op if the Mesh\n    is empty or has only one device.\n    \"\"\"\n    mesh = thread_resources.env.physical_mesh\n    if mesh.empty or mesh.size == 1:\n        return x\n    # TODO: jax types are bad\n    return jax.lax.with_sharding_constraint(x, shardings)  # type: ignore\n</code></pre>"},{"location":"api/sharding/presets/","title":"Sharding Presets","text":"<p>Loadax provides a few common sharding configurations that can be used out of the box. These presets are strong starting points for your sharding configuration, but you can also create your own sharding configurations using JAX's <code>Mesh</code> and <code>NamedSharding</code> primitives.</p>"},{"location":"api/sharding/presets/#fsdp-sharding","title":"FSDP Sharding","text":"<p>The FSDP sharding preset is a simple configuration that shards the model across multiple hosts and devices, and the data across multiple hosts. In FSDP training, the model parameters are split across multiple devices and each model shard recieves unique data. This configuration is useful for training models that are too large to fit on a single device.</p> Creating an FSDP sharding preset<pre><code>from loadax import Dataloader, SimpleDataset\nfrom loadax.sharding.placement import host_to_global_device_array\nfrom loadax.sharding.presets import make_fsdp_mesh_config\n\ndataset = SimpleDataset([jax.array([i]) for i in range(100)])\n\nmesh_config = make_fsdp_mesh_config(\n        mesh_axis_names=(\"data\", \"model\"), batch_axis_names=\"data\"\n)\nmesh = mesh_config.create_device_mesh()\n\ndataloader = Dataloader(dataset, batch_size=8)\n\n# Create your model, optimizer, metrics, and a train_step function\n# sharding your model parameters. See your framework's documentation\n# for how to configure FSDP or see examples/fsdp.py for an example \n# using flax's NNX api!\n...\n\nwith mesh:\n    for local_batch in dataloader:\n        # Stack the batch of arrays into a single array\n        local_batch = jnp.stack(local_batch)\n\n        # Convert the local batch to a global device array\n        global_batch = host_to_global_device_array(local_batch)\n\n        # Use jax.lax.with_sharding_constraint to specify the sharding of the input\n        sharded_batch = jax.lax.with_sharding_constraint(\n            global_batch, jax.sharding.PartitionSpec(mesh_rules.data)\n        )\n\n        # let jax.jit handle the movement of data across devices\n        loss = train_step(model, optimizer, metrics, sharded_batch)\n</code></pre> <p>Creates a MeshConfig configured for Fully Sharded Data Parallel (FSDP) training.</p> <ul> <li>Detects whether the execution is on a single node or multiple nodes.</li> <li>Determines the number of devices per node.</li> <li>Configures mesh shapes accordingly.</li> <li>Applies mesh rules based on the mesh_selector.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mesh_axis_names</code> <code>Sequence[str]</code> <p>The names of the mesh axes.</p> required <code>batch_axis_names</code> <code>str | Sequence[str]</code> <p>Subset of mesh axis names over which leaves of the input batch are sharded. Defaults to \"data\".</p> <code>'data'</code> <code>mesh_rules</code> <code>List[Tuple[str, MeshShape | HybridMeshShape]] | None</code> <p>Optional list of (regex, MeshShape) pairs to override the default mesh configuration based on the mesh_selector. Defaults to None.</p> <code>None</code> <code>mesh_selector</code> <code>str</code> <p>A string representing the hardware type or configuration, used to select the appropriate mesh rule. If None, no rules are applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MeshConfig</code> <code>MeshConfig</code> <p>The configured mesh configuration for FSDP.</p> Source code in <code>src/loadax/sharding/presets/fsdp.py</code> <pre><code>def make_fsdp_mesh_config(\n    mesh_axis_names: Sequence[str],\n    batch_axis_names: str | Sequence[str] = \"data\",\n    mesh_rules: list[tuple[str, MeshShape | HybridMeshShape | None]] | None = None,\n    mesh_selector: str | None = None,\n) -&gt; MeshConfig:\n    \"\"\"Creates a MeshConfig configured for Fully Sharded Data Parallel (FSDP) training.\n\n    - Detects whether the execution is on a single node or multiple nodes.\n    - Determines the number of devices per node.\n    - Configures mesh shapes accordingly.\n    - Applies mesh rules based on the mesh_selector.\n\n    Args:\n        mesh_axis_names (Sequence[str]): The names of the mesh axes.\n        batch_axis_names (str | Sequence[str], optional): Subset of mesh axis names\n            over which leaves of the input batch are sharded. Defaults to \"data\".\n        mesh_rules (List[Tuple[str, MeshShape | HybridMeshShape]] | None, optional):\n            Optional list of (regex, MeshShape) pairs to override the default mesh\n            configuration based on the mesh_selector. Defaults to None.\n        mesh_selector (str, optional): A string representing the hardware type or\n            configuration, used to select the appropriate mesh rule. If None, no rules\n            are applied.\n\n    Returns:\n        MeshConfig: The configured mesh configuration for FSDP.\n    \"\"\"\n    # Initialize default mesh_shape as None\n    default_mesh_shape: MeshShape | HybridMeshShape | None = None\n\n    # Apply mesh rules if provided\n    if mesh_rules and mesh_selector:\n        for pattern, shape in mesh_rules:\n            if re.match(pattern, mesh_selector):\n                if shape is None:\n                    raise ValueError(\n                        f\"Mesh shape for pattern '{pattern}' cannot be None.\"\n                    )\n                default_mesh_shape = shape\n                print(f\"Mesh rule matched: pattern='{pattern}', applying shape={shape}\")\n                break\n\n    # If no mesh_rule matched or no rules provided, infer mesh_shape\n    if default_mesh_shape is None:\n        # Total number of nodes participating in the computation\n        num_nodes = jax.process_count()\n\n        # Number of devices (e.g., GPUs) available on the current node\n        # we assume all nodes are homogeneous (jax assumes this as well)\n        devices_per_node = len(jax.local_devices())\n\n        if num_nodes &lt; 1:\n            raise ValueError(f\"Invalid number of nodes: {num_nodes}. Must be &gt;= 1.\")\n\n        if devices_per_node &lt; 1:\n            raise ValueError(f\"\"\"Invalid number of devices per node: {devices_per_node}.\n            Must be &gt;= 1.\"\"\")\n\n        # Configure DCN mesh shape\n        if num_nodes == 1:\n            # Single-node setup\n            dcn_mesh_shape = tuple([1] * len(mesh_axis_names))\n        else:\n            # Multi-node setup: assuming data or pipeline parallelism across nodes\n            # Identify the first axis to partition (first non-singleton)\n            # Here, we assume that the first axis is the one to be partitioned\n            # Modify this logic based on your specific parallelism strategy\n            dcn_mesh_shape = tuple([1] * len(mesh_axis_names))\n\n            # For simplicity, let's partition along the first axis\n            dcn_mesh_shape = (num_nodes,) + dcn_mesh_shape[1:]\n\n        # Configure ICI (Intra-Component Interconnect) mesh shape\n        ici_mesh_shape = list([1] * len(mesh_axis_names))\n        # Assume model parallelism is on the last axis\n        ici_mesh_shape[-1] = devices_per_node\n\n        hybrid_mesh_shape = HybridMeshShape(\n            ici_mesh_shape=tuple(ici_mesh_shape), dcn_mesh_shape=tuple(dcn_mesh_shape)\n        )\n\n        default_mesh_shape = hybrid_mesh_shape\n\n    # Instantiate MeshConfig\n    mesh_config = MeshConfig(\n        mesh_shape=default_mesh_shape,\n        mesh_axis_names=mesh_axis_names,\n        batch_axis_names=batch_axis_names,\n        mesh_rules=mesh_rules,  # type: ignore\n    )\n\n    return mesh_config\n</code></pre>"},{"location":"api/sharding/presets/#ddp-sharding","title":"DDP Sharding","text":"<p>The DDP sharding preset is a simple configuration that replicates the model across multiple devices and each replica recieves unique data. This configuration is ideal for training smaller models when you have multiple devices available.</p> DDP Sharding<pre><code>from loadax import Dataloader, SimpleDataset\nfrom loadax.sharding.placement import host_to_global_device_array\nfrom loadax.sharding.presets import make_ddp_mesh_config\n\ndataset = SimpleDataset([jax.array([i]) for i in range(100)])\n\nmesh_config = make_ddp_mesh_config(\n        mesh_axis_names=(\"data\",), batch_axis_names=\"data\"\n)\nmesh = mesh_config.create_device_mesh()\n\ndataloader = Dataloader(dataset, batch_size=8)\n\n# Create your model, optimizer, metrics, and a train_step function\n# letting jax.pmap handle replicate the model and sharding the data.\n...\n\nwith mesh:\n    for local_batch in dataloader:\n        # Stack the batch of arrays into a single array\n        local_batch = jnp.stack(local_batch)\n\n        # Convert the local batch to a global device array\n        global_batch = host_to_global_device_array(local_batch)\n\n        # Use jax.lax.with_sharding_constraint to specify the sharding of the input\n        sharded_batch = jax.lax.with_sharding_constraint(\n            global_batch, jax.sharding.PartitionSpec(mesh_rules.data)\n        )\n\n        # Use pmap to replicate the computation across all devices\n        loss = pmap_train_step(model, optimizer, metrics, sharded_batch)\n</code></pre> <p>Creates a MeshConfig configured for Data Parallel (DP) training.</p> <ul> <li>Detects whether the execution is on a single node or multiple nodes.</li> <li>Determines the total number of devices across all nodes.</li> <li>Configures mesh shapes for data parallelism.</li> <li>Applies mesh rules based on the mesh_selector.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mesh_axis_names</code> <code>Sequence[str]</code> <p>The names of the mesh axes. Defaults to (\"data\",).</p> <code>('data')</code> <code>batch_axis_names</code> <code>str | Sequence[str]</code> <p>Subset of mesh axis names over which leaves of the input batch are sharded. Defaults to \"data\".</p> <code>'data'</code> <code>mesh_rules</code> <code>List[Tuple[str, MeshShape | HybridMeshShape]] | None</code> <p>Optional list of (regex, MeshShape) pairs to override the default mesh configuration based on the mesh_selector. Defaults to None.</p> <code>None</code> <code>mesh_selector</code> <code>str</code> <p>A string representing the hardware type or configuration, used to select the appropriate mesh rule. If None, no rules are applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MeshConfig</code> <code>MeshConfig</code> <p>The configured mesh configuration for DP.</p> Source code in <code>src/loadax/sharding/presets/ddp.py</code> <pre><code>def make_ddp_mesh_config(\n    mesh_axis_names: Sequence[str] = (\"data\",),\n    batch_axis_names: str | Sequence[str] = \"data\",\n    mesh_rules: list[tuple[str, MeshShape | HybridMeshShape | None]] | None = None,\n    mesh_selector: str | None = None,\n) -&gt; MeshConfig:\n    \"\"\"Creates a MeshConfig configured for Data Parallel (DP) training.\n\n    - Detects whether the execution is on a single node or multiple nodes.\n    - Determines the total number of devices across all nodes.\n    - Configures mesh shapes for data parallelism.\n    - Applies mesh rules based on the mesh_selector.\n\n    Args:\n        mesh_axis_names (Sequence[str], optional): The names of the mesh axes.\n            Defaults to (\"data\",).\n        batch_axis_names (str | Sequence[str], optional): Subset of mesh axis names\n            over which leaves of the input batch are sharded. Defaults to \"data\".\n        mesh_rules (List[Tuple[str, MeshShape | HybridMeshShape]] | None, optional):\n            Optional list of (regex, MeshShape) pairs to override the default mesh\n            configuration based on the mesh_selector. Defaults to None.\n        mesh_selector (str, optional): A string representing the hardware type or\n            configuration, used to select the appropriate mesh rule. If None, no rules\n            are applied.\n\n    Returns:\n        MeshConfig: The configured mesh configuration for DP.\n    \"\"\"\n    # Initialize default mesh_shape as None\n    default_mesh_shape: MeshShape | HybridMeshShape | None = None\n\n    # Apply mesh rules if provided\n    if mesh_rules and mesh_selector:\n        for pattern, shape in mesh_rules:\n            if re.match(pattern, mesh_selector):\n                if shape is None:\n                    raise ValueError(\n                        f\"Mesh shape for pattern '{pattern}' cannot be None.\"\n                    )\n                default_mesh_shape = shape\n                print(f\"Mesh rule matched: pattern='{pattern}', applying shape={shape}\")\n                break\n\n    # If no mesh_rule matched or no rules provided, infer mesh_shape\n    if default_mesh_shape is None:\n        # Total number of nodes participating in the computation\n        num_nodes = jax.process_count()\n\n        # Number of devices (e.g., GPUs) available on the current node\n        # we assume all nodes are homogeneous (jax assumes this as well)\n        devices_per_node = len(jax.local_devices())\n\n        if num_nodes &lt; 1:\n            raise ValueError(f\"Invalid number of nodes: {num_nodes}. Must be &gt;= 1.\")\n\n        if devices_per_node &lt; 1:\n            raise ValueError(f\"\"\"Invalid number of devices per node: {devices_per_node}.\n            Must be &gt;= 1.\"\"\")\n\n        # For DP, we use a single axis for data parallelism\n        ici_mesh_shape = (devices_per_node,)\n        dcn_mesh_shape = (num_nodes,)\n\n        hybrid_mesh_shape = HybridMeshShape(\n            ici_mesh_shape=ici_mesh_shape, dcn_mesh_shape=dcn_mesh_shape\n        )\n\n        default_mesh_shape = hybrid_mesh_shape\n\n    # Instantiate MeshConfig\n    mesh_config = MeshConfig(\n        mesh_shape=default_mesh_shape,\n        mesh_axis_names=mesh_axis_names,\n        batch_axis_names=batch_axis_names,\n        mesh_rules=mesh_rules,  # type: ignore\n    )\n\n    return mesh_config\n</code></pre>"}]}